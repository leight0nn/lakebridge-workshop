"""
GlobalSupply Corp - SQL Server to Databricks Transpilation using Lakebridge

This script demonstrates automated SQL transpilation for simple and medium complexity
SQL files identified in Module 1 assessment as "Quick Wins" and "Standard Migration" candidates.

Business Context:
Building on Module 1's assessment findings, this script focuses on the 5 highest-priority
SQL files for migration (complexity scores 4.5-7.8) while leaving the most complex files
(8.5-9.8) as optional advanced exercises.

Target Files (from Module 1 Wave 1 & 2):
- financial_summary.sql (4.5/10) - Simple aggregation, basic joins
- order_processing.sql (5.1/10) - CRUD operations, transactions
- dynamic_reporting.sql (6.5/10) - Dynamic SQL, conditional logic
- window_functions_analysis.sql (7.2/10) - Advanced window functions
- customer_profitability.sql (7.8/10) - PIVOT operations, window functions

Usage:
    python 02_transpile_analyzer.py [--source-directory path] [--include-advanced]

Prerequisites:
    - Module 1 completed (sample SQL files available)
    - Databricks CLI installed and configured (optional)
    - Lakebridge installed: databricks labs install lakebridge (optional)
    - Python dependencies: pip install pandas sqlparse

Features:
    - Focuses on 5 core files identified in Module 1 assessment
    - Automated transpilation using Lakebridge when available
    - Manual conversion examples when Lakebridge unavailable
    - Detailed comparison reports and validation
    - Integration with Module 1 migration wave strategy
"""

import subprocess
import sys
import argparse
import os
from pathlib import Path
from datetime import datetime
import logging
from typing import List, Dict, Tuple, Optional
import json

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TranspilationAnalyzer:
    """
    Wrapper class for Lakebridge Transpiler functionality focused on
    Module 1's identified simple and medium complexity files
    """

    # Files to transpile (from Module 1 assessment)
    FOCUS_FILES = {
        # Simple complexity (Wave 1 - Quick Wins)
        'financial_summary.sql': {
            'complexity': 4.5,
            'hours': 4,
            'wave': 'Wave 1 - Quick Wins',
            'category': 'Reporting',
            'features': ['Basic Aggregation', 'Simple Joins']
        },
        'order_processing.sql': {
            'complexity': 5.1,
            'hours': 6,
            'wave': 'Wave 1 - Quick Wins',
            'category': 'OLTP',
            'features': ['CRUD Operations', 'Transactions']
        },

        # Medium complexity (Wave 2 - Standard Migration)
        'dynamic_reporting.sql': {
            'complexity': 6.5,
            'hours': 8,
            'wave': 'Wave 2 - Standard Migration',
            'category': 'Reporting',
            'features': ['Dynamic SQL', 'Conditional Logic']
        },
        'window_functions_analysis.sql': {
            'complexity': 7.2,
            'hours': 12,
            'wave': 'Wave 2 - Standard Migration',
            'category': 'Analytics',
            'features': ['Advanced Window Functions', 'LAG/LEAD']
        },
        'customer_profitability.sql': {
            'complexity': 7.8,
            'hours': 18,
            'wave': 'Wave 2 - Standard Migration',
            'category': 'Analytics',
            'features': ['PIVOT', 'Window Functions', 'String Aggregation']
        }
    }

    # Optional advanced files (Wave 3 - Complex Components)
    ADVANCED_FILES = {
        'supply_chain_performance.sql': {
            'complexity': 8.5,
            'hours': 16,
            'wave': 'Wave 3 - Complex Components',
            'category': 'Analytics',
            'features': ['CTEs', 'Window Functions', 'Complex Joins']
        },
        'inventory_optimization.sql': {
            'complexity': 9.2,
            'hours': 24,
            'wave': 'Wave 3 - Complex Components',
            'category': 'Analytics',
            'features': ['Recursive CTEs', 'PIVOT', 'Advanced Analytics']
        },
        'supplier_risk_assessment.sql': {
            'complexity': 9.8,
            'hours': 32,
            'wave': 'Wave 3 - Complex Components',
            'category': 'Analytics',
            'features': ['Recursive CTEs', 'Dynamic SQL', 'Risk Scoring']
        }
    }

    def __init__(self, source_directory: str = None, include_advanced: bool = False):
        self.source_directory = Path(source_directory) if source_directory else Path("../01_assessment/sample_sql")
        self.include_advanced = include_advanced
        self.output_directory = Path("transpiled_sql")
        self.report_directory = Path("reports")

        # Create output directories
        self.output_directory.mkdir(exist_ok=True)
        self.report_directory.mkdir(exist_ok=True)

        # Select files to process based on complexity focus
        self.target_files = self.FOCUS_FILES.copy()
        if include_advanced:
            self.target_files.update(self.ADVANCED_FILES)

    def validate_dependencies(self) -> bool:
        """
        Validate that required dependencies are available
        """
        try:
            import pandas
            import sqlparse
            logger.info("‚úÖ Required Python dependencies available")
            return True
        except ImportError as e:
            logger.error(f"‚ùå Missing Python dependencies: {e}")
            logger.error("Install with: pip install pandas sqlparse")
            return False

    def check_lakebridge_availability(self) -> bool:
        """
        Check if Lakebridge is installed and configured
        """
        try:
            result = subprocess.run(
                ["databricks", "labs", "lakebridge", "transpile", "--help"],
                capture_output=True,
                text=True,
                timeout=10
            )
            if result.returncode == 0:
                logger.info("‚úÖ Lakebridge Transpiler is available")
                return True
            else:
                logger.warning("‚ö†Ô∏è Lakebridge CLI available but transpiler may not be configured")
                return False
        except (subprocess.TimeoutExpired, FileNotFoundError):
            logger.warning("‚ö†Ô∏è Lakebridge not available - will use manual conversion examples")
            return False

    def validate_source_files(self) -> List[str]:
        """
        Validate that target SQL files exist and are accessible
        """
        available_files = []
        missing_files = []

        for filename in self.target_files.keys():
            file_path = self.source_directory / filename
            if file_path.exists():
                available_files.append(filename)
                file_info = self.target_files[filename]
                logger.info(f"‚úÖ {filename} - Complexity: {file_info['complexity']}/10, Wave: {file_info['wave']}")
            else:
                missing_files.append(filename)
                logger.error(f"‚ùå Missing: {filename}")

        if missing_files:
            logger.error(f"Missing {len(missing_files)} target files. Ensure Module 1 sample SQL files are available.")
            return []

        logger.info(f"üìÅ Found {len(available_files)} target files for transpilation")
        return available_files

    def transpile_with_lakebridge(self, filename: str) -> Tuple[bool, str, str]:
        """
        Attempt to transpile a single file using Lakebridge
        Returns: (success, transpiled_content, error_message)
        """
        source_path = self.source_directory / filename
        output_path = self.output_directory / filename.replace('.sql', '_databricks.sql')

        try:
            # Use Lakebridge CLI to transpile
            cmd = [
                "databricks", "labs", "lakebridge", "transpile",
                "--source-dialect", "tsql",
                "--input-source", str(source_path),
                "--output-folder", str(self.output_directory),
                "--skip-validation", "true"  # Skip validation for demo purposes
            ]

            logger.info(f"üîÑ Transpiling {filename} with Lakebridge...")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)

            if result.returncode == 0:
                # Read the transpiled output
                if output_path.exists():
                    with open(output_path, 'r') as f:
                        transpiled_content = f.read()
                    logger.info(f"‚úÖ Successfully transpiled {filename}")
                    return True, transpiled_content, ""
                else:
                    logger.warning(f"‚ö†Ô∏è Transpilation completed but output file not found: {filename}")
                    return False, "", "Output file not generated"
            else:
                error_msg = result.stderr or result.stdout or "Unknown transpilation error"
                logger.error(f"‚ùå Transpilation failed for {filename}: {error_msg}")
                return False, "", error_msg

        except subprocess.TimeoutExpired:
            error_msg = "Transpilation timeout"
            logger.error(f"‚ùå Timeout transpiling {filename}")
            return False, "", error_msg
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Error transpiling {filename}: {error_msg}")
            return False, "", error_msg

    def create_manual_conversion_example(self, filename: str) -> str:
        """
        Create manual conversion examples when Lakebridge is not available
        """
        source_path = self.source_directory / filename

        if not source_path.exists():
            return f"-- Error: Source file {filename} not found"

        # Read original SQL
        with open(source_path, 'r') as f:
            original_sql = f.read()

        # Create manual conversion with common patterns
        conversion_header = f"""/*
MANUAL CONVERSION EXAMPLE - {filename}
======================================
Original Complexity: {self.target_files[filename]['complexity']}/10
Migration Wave: {self.target_files[filename]['wave']}
Estimated Effort: {self.target_files[filename]['hours']} hours

COMMON CONVERSION PATTERNS APPLIED:
- SQL Server T-SQL ‚Üí Databricks SQL syntax
- Date functions updated for Spark SQL
- Window function syntax adjustments
- Transaction handling adapted for Delta Lake
- Variable declarations converted to session variables

NOTE: This is a demonstration conversion. Full production conversion
would require testing, optimization, and validation.
*/

-- DATABRICKS SQL VERSION
-- ======================
"""

        # Apply basic conversion patterns
        converted_sql = self._apply_basic_conversion_patterns(original_sql)

        return conversion_header + converted_sql

    def _apply_basic_conversion_patterns(self, sql_content: str) -> str:
        """
        Apply basic SQL Server to Databricks conversion patterns
        """
        # Basic pattern replacements (demonstration only)
        conversions = [
            # Date functions
            (r'GETDATE\(\)', 'CURRENT_TIMESTAMP()'),
            (r'DATEADD\(day,\s*([^,]+),\s*([^)]+)\)', r'DATE_ADD(\2, \1)'),
            (r'DATEDIFF\(day,\s*([^,]+),\s*([^)]+)\)', r'DATEDIFF(\2, \1)'),

            # Variable declarations (convert to session variables)
            (r'DECLARE\s+@(\w+)\s+(\w+)', r'-- Session variable: \1 (\2)'),

            # Transaction handling
            (r'BEGIN TRANSACTION[^;]*;', '-- BEGIN TRANSACTION (use Delta Lake ACID properties)'),
            (r'COMMIT TRANSACTION[^;]*;', '-- COMMIT (automatic with Delta Lake)'),
            (r'ROLLBACK TRANSACTION[^;]*;', '-- ROLLBACK (use Delta Lake versioning)'),

            # String functions
            (r'STRING_AGG\(([^,]+),\s*([^)]+)\)', r'ARRAY_JOIN(COLLECT_LIST(\1), \2)'),

            # System functions
            (r'@@ERROR', 'spark_error_code()'),
            (r'@@ROWCOUNT', 'spark_affected_rows()'),
            (r'SCOPE_IDENTITY\(\)', 'spark_last_insert_id()'),
        ]

        converted = sql_content
        for pattern, replacement in conversions:
            import re
            converted = re.sub(pattern, replacement, converted, flags=re.IGNORECASE)

        return converted

    def generate_transpilation_report(self, results: Dict) -> str:
        """
        Generate comprehensive transpilation report
        """
        total_files = len(results)
        successful = sum(1 for r in results.values() if r['success'])
        total_hours = sum(self.target_files[f]['hours'] for f in results.keys())

        report = f"""
GLOBALSUPPLY CORP - TRANSPILATION REPORT
========================================
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

SCOPE SUMMARY:
‚Ä¢ Files Processed: {total_files}
‚Ä¢ Successful Transpilations: {successful}
‚Ä¢ Total Estimated Effort: {total_hours} hours
‚Ä¢ Focus: Simple & Medium complexity (Module 1 Wave 1 & 2)

MIGRATION WAVE BREAKDOWN:
"""

        # Group by migration wave
        wave_stats = {}
        for filename, result in results.items():
            wave = self.target_files[filename]['wave']
            if wave not in wave_stats:
                wave_stats[wave] = {'count': 0, 'success': 0, 'hours': 0}
            wave_stats[wave]['count'] += 1
            wave_stats[wave]['hours'] += self.target_files[filename]['hours']
            if result['success']:
                wave_stats[wave]['success'] += 1

        for wave, stats in wave_stats.items():
            report += f"""
{wave}:
  ‚Ä¢ Files: {stats['count']} ({stats['success']} successful)
  ‚Ä¢ Estimated Hours: {stats['hours']}
  ‚Ä¢ Success Rate: {(stats['success']/stats['count']*100):.1f}%
"""

        report += f"""
DETAILED RESULTS:
"""
        for filename, result in results.items():
            file_info = self.target_files[filename]
            status = "‚úÖ SUCCESS" if result['success'] else "‚ùå FAILED"
            report += f"""
‚Ä¢ {filename} - {status}
  Complexity: {file_info['complexity']}/10 | Hours: {file_info['hours']} | Wave: {file_info['wave']}
  Features: {', '.join(file_info['features'])}"""

            if not result['success']:
                report += f"""
  Error: {result['error']}"""
            report += "\n"

        # Integration with Module 1
        report += f"""
INTEGRATION WITH MODULE 1 ASSESSMENT:
‚Ä¢ This transpilation focused on {total_files} files from Module 1's Wave 1 & 2 recommendations
‚Ä¢ Avoided high complexity files (Wave 3) to ensure manageable scope
‚Ä¢ Total effort: {total_hours} hours (vs {total_hours + 72} hours if including Wave 3)
‚Ä¢ Next step: Validate transpiled code in Module 3 (Data Reconciliation)

BUSINESS IMPACT:
‚Ä¢ Estimated Time Savings: 3-5x query performance improvement
‚Ä¢ Infrastructure Cost Reduction: 20-30% through cloud optimization
‚Ä¢ Analytics Capabilities: Advanced ML/AI features now available
‚Ä¢ User Experience: Natural language queries with Databricks Genie

NEXT STEPS:
1. Review transpiled SQL files in {self.output_directory}/
2. Run validation tests using 02_validation_tests.sql
3. Proceed to Module 3: Data Reconciliation for testing
4. Consider Wave 3 files as advanced exercises when ready
"""

        return report

    def run_transpilation_process(self) -> Dict:
        """
        Execute the complete transpilation process
        """
        logger.info("üöÄ Starting GlobalSupply Corp SQL Transpilation Process")
        logger.info(f"Target Scope: {len(self.target_files)} files (Complexity 4.5-7.8)")
        if self.include_advanced:
            logger.info("Including Wave 3 advanced files (Complexity 8.5-9.8)")

        # Step 1: Validate dependencies
        if not self.validate_dependencies():
            return {}

        # Step 2: Check Lakebridge availability
        lakebridge_available = self.check_lakebridge_availability()

        # Step 3: Validate source files
        available_files = self.validate_source_files()
        if not available_files:
            logger.error("‚ùå No source files available for transpilation")
            return {}

        # Step 4: Process each file
        results = {}
        for filename in available_files:
            if lakebridge_available:
                success, content, error = self.transpile_with_lakebridge(filename)
            else:
                # Fallback to manual conversion examples
                content = self.create_manual_conversion_example(filename)
                success = True
                error = ""

                # Save manual conversion
                output_path = self.output_directory / filename.replace('.sql', '_databricks.sql')
                with open(output_path, 'w') as f:
                    f.write(content)
                logger.info(f"üìù Created manual conversion example: {filename}")

            results[filename] = {
                'success': success,
                'content': content,
                'error': error,
                'method': 'lakebridge' if lakebridge_available else 'manual'
            }

        # Step 5: Generate comprehensive report
        report_content = self.generate_transpilation_report(results)
        report_path = self.report_directory / f"transpilation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

        with open(report_path, 'w') as f:
            f.write(report_content)

        logger.info(f"üìä Transpilation report generated: {report_path}")
        print(report_content)

        return results

def main():
    """
    Main function to orchestrate the transpilation process
    """
    parser = argparse.ArgumentParser(
        description="GlobalSupply Corp SQL Server to Databricks Transpilation (Module 2)"
    )
    parser.add_argument(
        "--source-directory",
        default="../01_assessment/sample_sql",
        help="Path to Module 1 sample SQL files"
    )
    parser.add_argument(
        "--include-advanced",
        action="store_true",
        help="Include Wave 3 high-complexity files (optional challenge)"
    )

    args = parser.parse_args()

    # Create analyzer instance
    analyzer = TranspilationAnalyzer(
        source_directory=args.source_directory,
        include_advanced=args.include_advanced
    )

    # Execute transpilation process
    results = analyzer.run_transpilation_process()

    if results:
        successful_count = sum(1 for r in results.values() if r['success'])
        total_count = len(results)

        logger.info(f"‚úÖ Transpilation completed: {successful_count}/{total_count} files successful")
        logger.info(f"üìÅ Output files: ./transpiled_sql/")
        logger.info(f"üìä Reports: ./reports/")
        logger.info(f"üöÄ Ready for Module 3: Data Reconciliation")
    else:
        logger.error("‚ùå Transpilation process failed")
        sys.exit(1)

if __name__ == "__main__":
    main()