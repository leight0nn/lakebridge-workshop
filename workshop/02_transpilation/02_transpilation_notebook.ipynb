{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GlobalSupply Corp - SQL Server to Databricks Transpilation\n",
        "\n",
        "## üöÄ Module 2: Schema Migration & Transpilation\n",
        "\n",
        "**Building on Module 1 Assessment Results**\n",
        "\n",
        "This notebook demonstrates the practical transpilation of SQL Server workloads to Databricks SQL, focusing on the **5 highest-priority files** identified in Module 1's assessment.\n",
        "\n",
        "### üìä Migration Strategy (From Module 1)\n",
        "- **Wave 1 - Quick Wins:** 2 files (Complexity ‚â§6.0)\n",
        "- **Wave 2 - Standard Migration:** 3 files (Complexity 6.1-7.9)  \n",
        "- **Wave 3 - Complex Components:** 3 files (Complexity ‚â•8.0) - *Optional advanced exercises*\n",
        "\n",
        "### üéØ Business Impact\n",
        "- **Cost Reduction:** Focus on manageable scope (44 hours vs 120 total hours)\n",
        "- **Risk Mitigation:** Start with proven success patterns\n",
        "- **Team Building:** Build confidence before tackling complex components\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Prerequisites & Setup\n",
        "\n",
        "### Step 1: Install Required Dependencies\n",
        "```bash\n",
        "# Core dependencies for transpilation analysis\n",
        "pip install pandas matplotlib seaborn sqlparse\n",
        "\n",
        "# Optional: Lakebridge for automated transpilation\n",
        "databricks labs install lakebridge\n",
        "```\n",
        "\n",
        "### Step 2: Databricks Environment Setup\n",
        "```bash\n",
        "# Configure Databricks CLI (if using real Databricks workspace)\n",
        "databricks configure\n",
        "\n",
        "# Create Unity Catalog structure (run in Databricks SQL)\n",
        "CREATE CATALOG IF NOT EXISTS globalsupply_corp;\n",
        "CREATE SCHEMA IF NOT EXISTS globalsupply_corp.raw;\n",
        "CREATE SCHEMA IF NOT EXISTS globalsupply_corp.analytics;\n",
        "```\n",
        "\n",
        "### Step 3: Verify Module 1 Completion\n",
        "Ensure the following files exist from Module 1:\n",
        "- `../01_assessment/sample_sql/*.sql` (8 sample files)\n",
        "- Assessment results and migration wave assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n",
        "print(\"üöÄ Ready for GlobalSupply Corp transpilation analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Module 1 Integration - Load Assessment Results\n",
        "\n",
        "First, let's load and review the assessment results from Module 1 to understand our transpilation scope and priorities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Module 1 assessment data and migration strategy\n",
        "# This integrates directly with the assessment findings\n",
        "\n",
        "# Define the 5 focus files based on Module 1 assessment\n",
        "FOCUS_FILES = {\n",
        "    # Wave 1 - Quick Wins (Simple complexity ‚â§6.0)\n",
        "    'financial_summary.sql': {\n",
        "        'complexity': 4.5,\n",
        "        'hours': 4,\n",
        "        'wave': 'Wave 1 - Quick Wins',\n",
        "        'category': 'Reporting',\n",
        "        'features': ['Basic Aggregation', 'Simple Joins'],\n",
        "        'priority': 'High - Build momentum'\n",
        "    },\n",
        "    'order_processing.sql': {\n",
        "        'complexity': 5.1,\n",
        "        'hours': 6,\n",
        "        'wave': 'Wave 1 - Quick Wins',\n",
        "        'category': 'OLTP',\n",
        "        'features': ['CRUD Operations', 'Transactions'],\n",
        "        'priority': 'High - Core business process'\n",
        "    },\n",
        "    \n",
        "    # Wave 2 - Standard Migration (Medium complexity 6.1-7.9)\n",
        "    'dynamic_reporting.sql': {\n",
        "        'complexity': 6.5,\n",
        "        'hours': 8,\n",
        "        'wave': 'Wave 2 - Standard Migration',\n",
        "        'category': 'Reporting',\n",
        "        'features': ['Dynamic SQL', 'Conditional Logic'],\n",
        "        'priority': 'Medium - Standard patterns'\n",
        "    },\n",
        "    'window_functions_analysis.sql': {\n",
        "        'complexity': 7.2,\n",
        "        'hours': 12,\n",
        "        'wave': 'Wave 2 - Standard Migration', \n",
        "        'category': 'Analytics',\n",
        "        'features': ['Advanced Window Functions', 'LAG/LEAD'],\n",
        "        'priority': 'Medium - Analytics foundation'\n",
        "    },\n",
        "    'customer_profitability.sql': {\n",
        "        'complexity': 7.8,\n",
        "        'hours': 18,\n",
        "        'wave': 'Wave 2 - Standard Migration',\n",
        "        'category': 'Analytics', \n",
        "        'features': ['PIVOT', 'Window Functions', 'String Aggregation'],\n",
        "        'priority': 'Medium - Advanced analytics'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Optional advanced files (Wave 3 - left as challenges)\n",
        "ADVANCED_FILES = {\n",
        "    'supply_chain_performance.sql': {'complexity': 8.5, 'hours': 16},\n",
        "    'inventory_optimization.sql': {'complexity': 9.2, 'hours': 24},\n",
        "    'supplier_risk_assessment.sql': {'complexity': 9.8, 'hours': 32}\n",
        "}\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "focus_df = pd.DataFrame.from_dict(FOCUS_FILES, orient='index').reset_index()\n",
        "focus_df.rename(columns={'index': 'filename'}, inplace=True)\n",
        "\n",
        "print(\"üìä MODULE 1 INTEGRATION - TRANSPILATION SCOPE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Focus Files: {len(FOCUS_FILES)} (Complexity 4.5-7.8)\")\n",
        "print(f\"Advanced Files: {len(ADVANCED_FILES)} (Complexity 8.5-9.8) - Optional\")\n",
        "print(f\"Total Effort: {focus_df['hours'].sum()} hours (vs {focus_df['hours'].sum() + 72} with advanced)\")\n",
        "print(f\"Cost Savings: ${(72 * 150):,} by focusing on manageable scope\\n\")\n",
        "\n",
        "# Display prioritized file list\n",
        "focus_df_display = focus_df[['filename', 'complexity', 'hours', 'wave', 'priority']].copy()\n",
        "focus_df_display['filename'] = focus_df_display['filename'].str.replace('.sql', '')\n",
        "display(focus_df_display.sort_values('complexity'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Validate Source Files and Environment\n",
        "\n",
        "Before starting transpilation, let's validate our environment and source files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check source files availability\n",
        "source_dir = Path('../01_assessment/sample_sql')\n",
        "missing_files = []\n",
        "available_files = []\n",
        "\n",
        "print(\"üìÅ VALIDATING SOURCE FILES\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for filename in FOCUS_FILES.keys():\n",
        "    file_path = source_dir / filename\n",
        "    if file_path.exists():\n",
        "        file_size = file_path.stat().st_size\n",
        "        available_files.append(filename)\n",
        "        file_info = FOCUS_FILES[filename]\n",
        "        print(f\"‚úÖ {filename} ({file_size:,} bytes)\")\n",
        "        print(f\"   Wave: {file_info['wave']} | Complexity: {file_info['complexity']}/10\")\n",
        "    else:\n",
        "        missing_files.append(filename)\n",
        "        print(f\"‚ùå Missing: {filename}\")\n",
        "\n",
        "print(f\"\\nüìä Status: {len(available_files)}/{len(FOCUS_FILES)} files available\")\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"‚ö†Ô∏è Missing files: {missing_files}\")\n",
        "    print(\"Please ensure Module 1 is completed with all sample SQL files.\")\n",
        "else:\n",
        "    print(\"üéâ All target files available for transpilation!\")\n",
        "\n",
        "# Check Lakebridge availability\n",
        "print(\"\\nüîß CHECKING TRANSPILATION TOOLS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"databricks\", \"labs\", \"lakebridge\", \"transpile\", \"--help\"],\n",
        "        capture_output=True, text=True, timeout=5\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        print(\"‚úÖ Lakebridge transpiler available\")\n",
        "        lakebridge_available = True\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Lakebridge installed but transpiler not configured\")\n",
        "        lakebridge_available = False\n",
        "except (subprocess.TimeoutExpired, FileNotFoundError):\n",
        "    print(\"‚ö†Ô∏è Lakebridge not available - will use manual conversion examples\")\n",
        "    lakebridge_available = False\n",
        "\n",
        "print(f\"\\nüöÄ Ready to proceed with {'automated' if lakebridge_available else 'manual'} transpilation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Execute Transpilation Process\n",
        "\n",
        "Now let's execute the transpilation process using our analyzer script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the transpilation analyzer\n",
        "print(\"üöÄ EXECUTING TRANSPILATION PROCESS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    # Run the transpilation analyzer script\n",
        "    cmd = [\"python\", \"02_transpile_analyzer.py\", \"--source-directory\", \"../01_assessment/sample_sql\"]\n",
        "    \n",
        "    print(f\"Executing: {' '.join(cmd)}\")\n",
        "    print(\"This may take a few moments...\\n\")\n",
        "    \n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"‚úÖ TRANSPILATION COMPLETED SUCCESSFULLY\")\n",
        "        print(result.stdout)\n",
        "        transpilation_success = True\n",
        "    else:\n",
        "        print(\"‚ùå TRANSPILATION ENCOUNTERED ISSUES\")\n",
        "        print(\"STDOUT:\", result.stdout)\n",
        "        print(\"STDERR:\", result.stderr)\n",
        "        transpilation_success = False\n",
        "\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"‚ùå Transpilation process timed out\")\n",
        "    transpilation_success = False\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error running transpilation: {e}\")\n",
        "    transpilation_success = False\n",
        "\n",
        "if not transpilation_success:\n",
        "    print(\"\\nüí° FALLBACK: Creating manual conversion examples...\")\n",
        "    print(\"In a real scenario, you would:\")\n",
        "    print(\"1. Review the error messages above\")\n",
        "    print(\"2. Fix configuration issues\")\n",
        "    print(\"3. Use manual conversion patterns from 02_manual_conversion_guide.sql\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Analyze Transpilation Results\n",
        "\n",
        "Let's examine the transpiled files and compare them with the originals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and analyze transpiled files\n",
        "transpiled_dir = Path('transpiled_sql')\n",
        "source_dir = Path('../01_assessment/sample_sql')\n",
        "\n",
        "print(\"üìä TRANSPILATION RESULTS ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "results_data = []\n",
        "\n",
        "for original_filename in FOCUS_FILES.keys():\n",
        "    original_path = source_dir / original_filename\n",
        "    transpiled_filename = original_filename.replace('.sql', '_databricks.sql')\n",
        "    transpiled_path = transpiled_dir / transpiled_filename\n",
        "    \n",
        "    file_info = FOCUS_FILES[original_filename]\n",
        "    \n",
        "    # Check if files exist and get sizes\n",
        "    original_exists = original_path.exists()\n",
        "    transpiled_exists = transpiled_path.exists()\n",
        "    \n",
        "    original_size = original_path.stat().st_size if original_exists else 0\n",
        "    transpiled_size = transpiled_path.stat().st_size if transpiled_exists else 0\n",
        "    \n",
        "    # Read file content for analysis (first 500 chars for preview)\n",
        "    original_preview = \"\"\n",
        "    transpiled_preview = \"\"\n",
        "    \n",
        "    if original_exists:\n",
        "        with open(original_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            original_preview = content[:500] + \"...\" if len(content) > 500 else content\n",
        "    \n",
        "    if transpiled_exists:\n",
        "        with open(transpiled_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            transpiled_preview = content[:500] + \"...\" if len(content) > 500 else content\n",
        "    \n",
        "    results_data.append({\n",
        "        'filename': original_filename,\n",
        "        'complexity': file_info['complexity'],\n",
        "        'wave': file_info['wave'],\n",
        "        'original_size': original_size,\n",
        "        'transpiled_size': transpiled_size,\n",
        "        'transpiled_exists': transpiled_exists,\n",
        "        'size_change_pct': ((transpiled_size - original_size) / original_size * 100) if original_size > 0 else 0,\n",
        "        'features': file_info['features']\n",
        "    })\n",
        "    \n",
        "    # Display individual file results\n",
        "    status = \"‚úÖ SUCCESS\" if transpiled_exists else \"‚ùå NOT FOUND\"\n",
        "    print(f\"\\n{status} {original_filename}\")\n",
        "    print(f\"  Complexity: {file_info['complexity']}/10 | Wave: {file_info['wave']}\")\n",
        "    print(f\"  Original: {original_size:,} bytes | Transpiled: {transpiled_size:,} bytes\")\n",
        "    if transpiled_exists and original_size > 0:\n",
        "        change_pct = (transpiled_size - original_size) / original_size * 100\n",
        "        print(f\"  Size Change: {change_pct:+.1f}% (includes comments and optimizations)\")\n",
        "\n",
        "# Create summary DataFrame\n",
        "results_df = pd.DataFrame(results_data)\n",
        "successful_files = results_df[results_df['transpiled_exists']]\n",
        "\n",
        "print(f\"\\nüìà OVERALL RESULTS:\")\n",
        "print(f\"  Files Processed: {len(results_df)}\")\n",
        "print(f\"  Successful Transpilations: {len(successful_files)}\")\n",
        "print(f\"  Success Rate: {len(successful_files)/len(results_df)*100:.1f}%\")\n",
        "print(f\"  Total Original Size: {results_df['original_size'].sum():,} bytes\")\n",
        "print(f\"  Total Transpiled Size: {successful_files['transpiled_size'].sum():,} bytes\")\n",
        "\n",
        "if len(successful_files) > 0:\n",
        "    avg_size_change = successful_files['size_change_pct'].mean()\n",
        "    print(f\"  Average Size Change: {avg_size_change:+.1f}% (includes documentation)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualize Transpilation Analysis\n",
        "\n",
        "Create visualizations to understand the transpilation results and complexity distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive transpilation analysis dashboard\n",
        "if len(results_df) > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('GlobalSupply Corp - Transpilation Analysis Dashboard', \n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "    \n",
        "    # 1. Success rate by migration wave\n",
        "    wave_success = results_df.groupby('wave').agg({\n",
        "        'transpiled_exists': ['count', 'sum']\n",
        "    }).round(2)\n",
        "    wave_success.columns = ['Total', 'Successful']\n",
        "    wave_success['Success_Rate'] = (wave_success['Successful'] / wave_success['Total'] * 100)\n",
        "    \n",
        "    wave_names = [w.replace(' - ', '\\n') for w in wave_success.index]\n",
        "    bars1 = axes[0, 0].bar(wave_names, wave_success['Success_Rate'], \n",
        "                          color=['#2ecc71', '#f39c12'], alpha=0.8)\n",
        "    axes[0, 0].set_title('üåä Success Rate by Migration Wave', fontweight='bold')\n",
        "    axes[0, 0].set_ylabel('Success Rate (%)')\n",
        "    axes[0, 0].set_ylim(0, 105)\n",
        "    \n",
        "    # Add percentage labels on bars\n",
        "    for bar, pct in zip(bars1, wave_success['Success_Rate']):\n",
        "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                        f'{pct:.0f}%', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 2. Complexity vs File Size Analysis\n",
        "    successful_only = results_df[results_df['transpiled_exists']]\n",
        "    if len(successful_only) > 0:\n",
        "        scatter = axes[0, 1].scatter(successful_only['complexity'], \n",
        "                                    successful_only['original_size'],\n",
        "                                    c=successful_only['transpiled_size'], \n",
        "                                    cmap='viridis', alpha=0.8, s=150, \n",
        "                                    edgecolors='black', linewidth=0.5)\n",
        "        \n",
        "        axes[0, 1].set_xlabel('Complexity Score')\n",
        "        axes[0, 1].set_ylabel('Original File Size (bytes)')\n",
        "        axes[0, 1].set_title('üìä Complexity vs File Size\\n(Color = Transpiled Size)', fontweight='bold')\n",
        "        \n",
        "        # Add colorbar\n",
        "        cbar = plt.colorbar(scatter, ax=axes[0, 1])\n",
        "        cbar.set_label('Transpiled Size (bytes)', rotation=270, labelpad=20)\n",
        "    \n",
        "    # 3. File size changes\n",
        "    if len(successful_only) > 0:\n",
        "        file_names = [f.replace('.sql', '') for f in successful_only['filename']]\n",
        "        size_changes = successful_only['size_change_pct']\n",
        "        \n",
        "        colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in size_changes]\n",
        "        bars3 = axes[1, 0].barh(file_names, size_changes, color=colors, alpha=0.7)\n",
        "        \n",
        "        axes[1, 0].set_xlabel('Size Change (%)')\n",
        "        axes[1, 0].set_title('üìè File Size Changes After Transpilation', fontweight='bold')\n",
        "        axes[1, 0].axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
        "        \n",
        "        # Add percentage labels\n",
        "        for i, (bar, pct) in enumerate(zip(bars3, size_changes)):\n",
        "            axes[1, 0].text(pct + (5 if pct >= 0 else -5), i,\n",
        "                            f'{pct:+.0f}%', va='center', \n",
        "                            ha='left' if pct >= 0 else 'right', fontweight='bold')\n",
        "    \n",
        "    # 4. Migration timeline and effort\n",
        "    wave_effort = focus_df.groupby('wave')['hours'].sum()\n",
        "    \n",
        "    # Create timeline visualization\n",
        "    timeline_data = {\n",
        "        'Wave 1 - Quick Wins': {'start': 0, 'duration': 2, 'hours': wave_effort.get('Wave 1 - Quick Wins', 0)},\n",
        "        'Wave 2 - Standard Migration': {'start': 1, 'duration': 6, 'hours': wave_effort.get('Wave 2 - Standard Migration', 0)}\n",
        "    }\n",
        "    \n",
        "    y_pos = 0\n",
        "    colors_timeline = ['#2ecc71', '#f39c12']\n",
        "    \n",
        "    for i, (wave, data) in enumerate(timeline_data.items()):\n",
        "        axes[1, 1].barh(y_pos, data['duration'], left=data['start'], \n",
        "                       color=colors_timeline[i], alpha=0.7, height=0.6)\n",
        "        \n",
        "        # Add wave and hours labels\n",
        "        axes[1, 1].text(data['start'] + data['duration']/2, y_pos,\n",
        "                        f\"{wave.split(' - ')[0]}\\n{data['hours']}h\",\n",
        "                        ha='center', va='center', fontweight='bold', color='white')\n",
        "        y_pos += 1\n",
        "    \n",
        "    axes[1, 1].set_xlim(0, 8)\n",
        "    axes[1, 1].set_ylim(-0.5, 1.5)\n",
        "    axes[1, 1].set_xlabel('Timeline (Weeks)')\n",
        "    axes[1, 1].set_title('üìÖ Migration Timeline & Effort', fontweight='bold')\n",
        "    axes[1, 1].set_yticks(range(2))\n",
        "    axes[1, 1].set_yticklabels(['Wave 1', 'Wave 2'])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.93)\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No transpilation results available for visualization\")\n",
        "    print(\"Please ensure the transpilation process completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Before/After Code Comparison\n",
        "\n",
        "Let's examine specific examples of transpilation to understand the key changes made."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to display before/after code comparison\n",
        "def show_code_comparison(filename, max_lines=30):\n",
        "    \"\"\"\n",
        "    Display side-by-side comparison of original vs transpiled SQL\n",
        "    \"\"\"\n",
        "    original_path = source_dir / filename\n",
        "    transpiled_path = transpiled_dir / filename.replace('.sql', '_databricks.sql')\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üìã CODE COMPARISON: {filename}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    if original_path.exists() and transpiled_path.exists():\n",
        "        # Read files\n",
        "        with open(original_path, 'r') as f:\n",
        "            original_lines = f.readlines()[:max_lines]\n",
        "        \n",
        "        with open(transpiled_path, 'r') as f:\n",
        "            transpiled_lines = f.readlines()[:max_lines]\n",
        "        \n",
        "        print(f\"{'ORIGINAL (SQL Server)':^40} | {'TRANSPILED (Databricks)':^40}\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        # Display line by line comparison\n",
        "        max_len = max(len(original_lines), len(transpiled_lines))\n",
        "        \n",
        "        for i in range(min(max_lines, max_len)):\n",
        "            orig_line = original_lines[i].rstrip() if i < len(original_lines) else \"\"\n",
        "            trans_line = transpiled_lines[i].rstrip() if i < len(transpiled_lines) else \"\"\n",
        "            \n",
        "            # Truncate long lines\n",
        "            orig_line = orig_line[:38] + \"..\" if len(orig_line) > 40 else orig_line\n",
        "            trans_line = trans_line[:38] + \"..\" if len(trans_line) > 40 else trans_line\n",
        "            \n",
        "            print(f\"{orig_line:<40} | {trans_line:<40}\")\n",
        "        \n",
        "        if max_len > max_lines:\n",
        "            print(f\"... ({max_len - max_lines} more lines) ...\")\n",
        "            \n",
        "    else:\n",
        "        print(\"‚ùå One or both files not found\")\n",
        "        print(f\"Original exists: {original_path.exists()}\")\n",
        "        print(f\"Transpiled exists: {transpiled_path.exists()}\")\n",
        "\n",
        "# Show comparisons for successfully transpiled files\n",
        "print(\"üîç BEFORE/AFTER CODE COMPARISONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Start with the simplest file\n",
        "comparison_files = ['financial_summary.sql', 'order_processing.sql']\n",
        "\n",
        "for filename in comparison_files:\n",
        "    if filename in FOCUS_FILES:\n",
        "        show_code_comparison(filename, max_lines=25)\n",
        "        \n",
        "        # Highlight key changes\n",
        "        file_info = FOCUS_FILES[filename]\n",
        "        print(f\"\\nüîß KEY CHANGES for {filename}:\")\n",
        "        \n",
        "        if filename == 'financial_summary.sql':\n",
        "            print(\"  ‚Ä¢ Added Unity Catalog references (globalsupply_corp.raw.*)\")\n",
        "            print(\"  ‚Ä¢ Added performance optimization comments\")\n",
        "            print(\"  ‚Ä¢ Minimal syntax changes (already ANSI compatible)\")\n",
        "        \n",
        "        elif filename == 'order_processing.sql':\n",
        "            print(\"  ‚Ä¢ GETDATE() ‚Üí CURRENT_TIMESTAMP()\")\n",
        "            print(\"  ‚Ä¢ DATEADD(day, N, date) ‚Üí DATE_ADD(date, N)\")\n",
        "            print(\"  ‚Ä¢ DECLARE @var ‚Üí DECLARE var (session variables)\")\n",
        "            print(\"  ‚Ä¢ SCOPE_IDENTITY() ‚Üí MAX(column) approach\")\n",
        "            print(\"  ‚Ä¢ BEGIN/COMMIT TRANSACTION ‚Üí Delta Lake ACID\")\n",
        "            print(\"  ‚Ä¢ Added MERGE operations for better performance\")\n",
        "\n",
        "print(\"\\nüí° For complete comparisons, examine files in ./transpiled_sql/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Key Conversion Patterns Summary\n",
        "\n",
        "Based on the transpilation analysis, let's summarize the most common conversion patterns encountered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze and summarize conversion patterns\n",
        "print(\"üìä KEY CONVERSION PATTERNS IDENTIFIED\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "conversion_patterns = {\n",
        "    \"Date Functions\": {\n",
        "        \"files_affected\": [\"order_processing.sql\", \"dynamic_reporting.sql\"],\n",
        "        \"changes\": [\n",
        "            \"GETDATE() ‚Üí CURRENT_TIMESTAMP()\",\n",
        "            \"DATEADD(day, N, date) ‚Üí DATE_ADD(date, N)\",\n",
        "            \"DATEDIFF(day, date1, date2) ‚Üí DATEDIFF(date2, date1)\"\n",
        "        ],\n",
        "        \"complexity\": \"Simple\",\n",
        "        \"risk\": \"Low\"\n",
        "    },\n",
        "    \n",
        "    \"Variable Declarations\": {\n",
        "        \"files_affected\": [\"order_processing.sql\", \"dynamic_reporting.sql\"],\n",
        "        \"changes\": [\n",
        "            \"DECLARE @var TYPE ‚Üí DECLARE var TYPE\",\n",
        "            \"Set session variables instead of local variables\",\n",
        "            \"Consider stored procedure parameters\"\n",
        "        ],\n",
        "        \"complexity\": \"Medium\",\n",
        "        \"risk\": \"Medium\"\n",
        "    },\n",
        "    \n",
        "    \"Transaction Handling\": {\n",
        "        \"files_affected\": [\"order_processing.sql\"],\n",
        "        \"changes\": [\n",
        "            \"BEGIN TRANSACTION ‚Üí BEGIN (Delta Lake ACID)\",\n",
        "            \"COMMIT/ROLLBACK ‚Üí Automatic with Delta Lake\",\n",
        "            \"Use MERGE for multi-table operations\"\n",
        "        ],\n",
        "        \"complexity\": \"High\", \n",
        "        \"risk\": \"High\"\n",
        "    },\n",
        "    \n",
        "    \"String Aggregation\": {\n",
        "        \"files_affected\": [\"customer_profitability.sql\"],\n",
        "        \"changes\": [\n",
        "            \"STRING_AGG(col, delimiter) ‚Üí ARRAY_JOIN(COLLECT_LIST(col), delimiter)\"\n",
        "        ],\n",
        "        \"complexity\": \"Medium\",\n",
        "        \"risk\": \"Low\"\n",
        "    },\n",
        "    \n",
        "    \"Window Functions\": {\n",
        "        \"files_affected\": [\"window_functions_analysis.sql\", \"customer_profitability.sql\"],\n",
        "        \"changes\": [\n",
        "            \"Most syntax identical\",\n",
        "            \"Frame specifications work the same\",\n",
        "            \"Performance optimization opportunities\"\n",
        "        ],\n",
        "        \"complexity\": \"Simple\",\n",
        "        \"risk\": \"Low\"\n",
        "    },\n",
        "    \n",
        "    \"Schema References\": {\n",
        "        \"files_affected\": [\"All files\"],\n",
        "        \"changes\": [\n",
        "            \"table_name ‚Üí catalog.schema.table_name\",\n",
        "            \"Added Unity Catalog structure\",\n",
        "            \"Namespace organization\"\n",
        "        ],\n",
        "        \"complexity\": \"Simple\",\n",
        "        \"risk\": \"Low\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display pattern analysis\n",
        "for pattern_name, details in conversion_patterns.items():\n",
        "    print(f\"\\nüîß {pattern_name.upper()}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Files Affected: {len(details['files_affected'])} - {', '.join(details['files_affected'][:2])}{'...' if len(details['files_affected']) > 2 else ''}\")\n",
        "    print(f\"Complexity: {details['complexity']} | Risk: {details['risk']}\")\n",
        "    print(\"Key Changes:\")\n",
        "    for change in details['changes']:\n",
        "        print(f\"  ‚Ä¢ {change}\")\n",
        "\n",
        "# Create pattern complexity distribution\n",
        "pattern_df = pd.DataFrame([\n",
        "    {'pattern': k, 'complexity': v['complexity'], 'risk': v['risk'], 'files': len(v['files_affected'])}\n",
        "    for k, v in conversion_patterns.items()\n",
        "])\n",
        "\n",
        "# Visualization of pattern complexity and risk\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Complexity distribution\n",
        "complexity_counts = pattern_df['complexity'].value_counts()\n",
        "colors_comp = {'Simple': '#2ecc71', 'Medium': '#f39c12', 'High': '#e74c3c'}\n",
        "comp_colors = [colors_comp[comp] for comp in complexity_counts.index]\n",
        "\n",
        "wedges1, texts1, autotexts1 = ax1.pie(complexity_counts.values, labels=complexity_counts.index,\n",
        "                                      autopct='%1.0f%%', colors=comp_colors, startangle=90)\n",
        "ax1.set_title('üéØ Pattern Complexity Distribution', fontweight='bold')\n",
        "\n",
        "# Risk distribution  \n",
        "risk_counts = pattern_df['risk'].value_counts()\n",
        "colors_risk = {'Low': '#2ecc71', 'Medium': '#f39c12', 'High': '#e74c3c'}\n",
        "risk_colors = [colors_risk[risk] for risk in risk_counts.index]\n",
        "\n",
        "wedges2, texts2, autotexts2 = ax2.pie(risk_counts.values, labels=risk_counts.index,\n",
        "                                      autopct='%1.0f%%', colors=risk_colors, startangle=90)\n",
        "ax2.set_title('‚ö†Ô∏è Pattern Risk Distribution', fontweight='bold')\n",
        "\n",
        "plt.suptitle('Conversion Pattern Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìà PATTERN INSIGHTS:\")\n",
        "print(f\"  Total Patterns: {len(conversion_patterns)}\")\n",
        "print(f\"  Simple Patterns: {len(pattern_df[pattern_df['complexity'] == 'Simple'])} (Low effort)\")\n",
        "print(f\"  Medium Patterns: {len(pattern_df[pattern_df['complexity'] == 'Medium'])} (Standard effort)\")\n",
        "print(f\"  High Patterns: {len(pattern_df[pattern_df['complexity'] == 'High'])} (Expert required)\")\n",
        "print(f\"  Low Risk: {len(pattern_df[pattern_df['risk'] == 'Low'])} patterns\")\n",
        "print(f\"  High Risk: {len(pattern_df[pattern_df['risk'] == 'High'])} patterns (require careful testing)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Validation & Testing Strategy\n",
        "\n",
        "Now let's define our approach for validating the transpiled SQL code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define validation strategy based on transpilation results\n",
        "print(\"‚úÖ VALIDATION & TESTING STRATEGY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "validation_plan = {\n",
        "    \"Phase 1 - Syntax Validation\": {\n",
        "        \"description\": \"Verify transpiled SQL parses correctly\",\n",
        "        \"tools\": [\"Databricks SQL Parser\", \"SQLFluff\", \"Manual Review\"],\n",
        "        \"files\": \"All transpiled files\",\n",
        "        \"effort\": \"2-4 hours\",\n",
        "        \"risk\": \"Low\"\n",
        "    },\n",
        "    \n",
        "    \"Phase 2 - Schema Compatibility\": {\n",
        "        \"description\": \"Ensure Unity Catalog references are correct\", \n",
        "        \"tools\": [\"Unity Catalog DDL\", \"Information Schema Queries\"],\n",
        "        \"files\": \"All files with table references\",\n",
        "        \"effort\": \"4-6 hours\",\n",
        "        \"risk\": \"Medium\"\n",
        "    },\n",
        "    \n",
        "    \"Phase 3 - Data Reconciliation\": {\n",
        "        \"description\": \"Compare results between SQL Server and Databricks\",\n",
        "        \"tools\": [\"Lakebridge Reconcile\", \"Custom Validation Queries\"],\n",
        "        \"files\": \"Simple files first (financial_summary.sql)\",\n",
        "        \"effort\": \"8-12 hours\", \n",
        "        \"risk\": \"High\"\n",
        "    },\n",
        "    \n",
        "    \"Phase 4 - Performance Testing\": {\n",
        "        \"description\": \"Validate performance improvements\",\n",
        "        \"tools\": [\"Databricks SQL Analytics\", \"Query Plans\", \"Spark UI\"],\n",
        "        \"files\": \"Analytics files (window_functions, customer_profitability)\",\n",
        "        \"effort\": \"6-8 hours\",\n",
        "        \"risk\": \"Medium\"\n",
        "    },\n",
        "    \n",
        "    \"Phase 5 - Business Logic Validation\": {\n",
        "        \"description\": \"Verify business calculations and logic\",\n",
        "        \"tools\": [\"Business User Review\", \"Sample Data Testing\"],\n",
        "        \"files\": \"Business-critical files (order_processing.sql)\",\n",
        "        \"effort\": \"10-15 hours\",\n",
        "        \"risk\": \"High\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display validation plan\n",
        "total_effort_hours = 0\n",
        "high_risk_phases = 0\n",
        "\n",
        "for phase_name, details in validation_plan.items():\n",
        "    print(f\"\\nüìã {phase_name}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Description: {details['description']}\")\n",
        "    print(f\"Tools: {', '.join(details['tools'])}\")\n",
        "    print(f\"Scope: {details['files']}\")\n",
        "    print(f\"Effort: {details['effort']} | Risk: {details['risk']}\")\n",
        "    \n",
        "    # Extract effort hours for summary\n",
        "    effort_str = details['effort'].split('-')[1] if '-' in details['effort'] else details['effort']\n",
        "    effort_hours = int(''.join(filter(str.isdigit, effort_str)))\n",
        "    total_effort_hours += effort_hours\n",
        "    \n",
        "    if details['risk'] == 'High':\n",
        "        high_risk_phases += 1\n",
        "\n",
        "print(f\"\\nüìä VALIDATION SUMMARY:\")\n",
        "print(f\"  Total Phases: {len(validation_plan)}\")\n",
        "print(f\"  Estimated Effort: ~{total_effort_hours} hours\")\n",
        "print(f\"  High Risk Phases: {high_risk_phases}\")\n",
        "print(f\"  Recommended Timeline: 2-3 weeks\")\n",
        "\n",
        "# Create simple validation checklist\n",
        "print(f\"\\n‚úÖ IMMEDIATE VALIDATION CHECKLIST:\")\n",
        "checklist = [\n",
        "    \"Review transpiled files for obvious syntax errors\",\n",
        "    \"Check Unity Catalog references are consistent\", \n",
        "    \"Verify date function conversions are correct\",\n",
        "    \"Test simple queries first (financial_summary.sql)\",\n",
        "    \"Document any manual fixes needed\",\n",
        "    \"Plan data reconciliation tests (Module 3)\"\n",
        "]\n",
        "\n",
        "for i, item in enumerate(checklist, 1):\n",
        "    print(f\"  {i}. [ ] {item}\")\n",
        "\n",
        "print(\"\\nüöÄ Next Step: Execute validation tests using 02_validation_tests.sql\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì§ Export Results and Generate Deployment Artifacts\n",
        "\n",
        "Create deployment-ready artifacts and documentation for the transpiled SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive results and deployment artifacts\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "print(\"üì§ GENERATING DEPLOYMENT ARTIFACTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create deployment summary\n",
        "deployment_summary = {\n",
        "    \"project\": \"GlobalSupply Corp - SQL Server to Databricks Migration\",\n",
        "    \"module\": \"Module 2 - Schema Migration & Transpilation\", \n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"scope\": {\n",
        "        \"target_files\": len(FOCUS_FILES),\n",
        "        \"complexity_range\": \"4.5-7.8 (Simple to Medium)\",\n",
        "        \"migration_waves\": [\"Wave 1 - Quick Wins\", \"Wave 2 - Standard Migration\"],\n",
        "        \"estimated_effort\": f\"{focus_df['hours'].sum()} hours\"\n",
        "    },\n",
        "    \"results\": {\n",
        "        \"files_processed\": len(results_df) if 'results_df' in locals() else 0,\n",
        "        \"successful_transpilations\": len(successful_files) if 'successful_files' in locals() else 0,\n",
        "        \"transpilation_method\": \"lakebridge\" if lakebridge_available else \"manual\",\n",
        "    },\n",
        "    \"files\": dict(FOCUS_FILES)\n",
        "}\n",
        "\n",
        "# Save deployment summary\n",
        "summary_path = Path('deployment_summary.json')\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(deployment_summary, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Deployment summary saved: {summary_path}\")\n",
        "\n",
        "# Generate README for transpiled files\n",
        "readme_content = f\"\"\"\n",
        "# GlobalSupply Corp - Transpiled SQL Files\n",
        "\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "Module: 2 - Schema Migration & Transpilation\n",
        "\n",
        "## üìÅ Files Overview\n",
        "\n",
        "This directory contains SQL Server workloads transpiled to Databricks SQL syntax.\n",
        "\n",
        "### Migration Waves (from Module 1 Assessment)\n",
        "\n",
        "**Wave 1 - Quick Wins** (Simple complexity ‚â§6.0):\n",
        "- `financial_summary_databricks.sql` - Basic aggregation and reporting\n",
        "- `order_processing_databricks.sql` - CRUD operations with transactions\n",
        "\n",
        "**Wave 2 - Standard Migration** (Medium complexity 6.1-7.9):\n",
        "- `dynamic_reporting_databricks.sql` - Dynamic SQL and conditional logic\n",
        "- `window_functions_analysis_databricks.sql` - Advanced window functions\n",
        "- `customer_profitability_databricks.sql` - PIVOT operations and analytics\n",
        "\n",
        "## üöÄ Deployment Instructions\n",
        "\n",
        "### Prerequisites\n",
        "1. Databricks workspace with Unity Catalog enabled\n",
        "2. Create catalog and schemas:\n",
        "   ```sql\n",
        "   CREATE CATALOG IF NOT EXISTS globalsupply_corp;\n",
        "   CREATE SCHEMA IF NOT EXISTS globalsupply_corp.raw;\n",
        "   CREATE SCHEMA IF NOT EXISTS globalsupply_corp.analytics;\n",
        "   ```\n",
        "\n",
        "### Deployment Order\n",
        "1. Start with Wave 1 files (lowest complexity)\n",
        "2. Validate results before proceeding to Wave 2\n",
        "3. Run validation tests from `../02_validation_tests.sql`\n",
        "\n",
        "### Key Changes Made\n",
        "- **Date Functions**: `GETDATE()` ‚Üí `CURRENT_TIMESTAMP()`\n",
        "- **Schema References**: Added Unity Catalog namespacing\n",
        "- **Transactions**: Adapted for Delta Lake ACID properties\n",
        "- **Variables**: Converted to session variables or stored procedure parameters\n",
        "- **Performance**: Added optimization hints and suggestions\n",
        "\n",
        "## ‚ö†Ô∏è Important Notes\n",
        "- All files include validation status in headers\n",
        "- High-risk changes are clearly marked\n",
        "- Performance optimization suggestions provided\n",
        "- Test thoroughly before production deployment\n",
        "\n",
        "## üìû Next Steps\n",
        "1. Review each file's header comments for specific changes\n",
        "2. Run syntax validation in Databricks SQL\n",
        "3. Execute validation tests\n",
        "4. Proceed to Module 3: Data Reconciliation\n",
        "\n",
        "For questions or issues, refer to the transpilation analysis notebook.\n",
        "\"\"\"\n",
        "\n",
        "readme_path = Path('transpiled_sql/README.md')\n",
        "readme_path.parent.mkdir(exist_ok=True)\n",
        "with open(readme_path, 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(f\"‚úÖ README file generated: {readme_path}\")\n",
        "\n",
        "# Create deployment checklist\n",
        "checklist_content = f\"\"\"\n",
        "GlobalSupply Corp - Module 2 Deployment Checklist\n",
        "================================================\n",
        "\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "PRE-DEPLOYMENT VALIDATION:\n",
        "[ ] All {len(FOCUS_FILES)} target files successfully transpiled\n",
        "[ ] Syntax validation completed without errors\n",
        "[ ] Unity Catalog schema structure created\n",
        "[ ] Sample data available for testing\n",
        "[ ] Databricks SQL warehouse configured\n",
        "\n",
        "DEPLOYMENT SEQUENCE:\n",
        "\n",
        "WAVE 1 - QUICK WINS:\n",
        "[ ] Deploy financial_summary_databricks.sql\n",
        "[ ] Test with sample data\n",
        "[ ] Validate results against original\n",
        "[ ] Deploy order_processing_databricks.sql  \n",
        "[ ] Test transaction handling\n",
        "[ ] Document any issues found\n",
        "\n",
        "WAVE 2 - STANDARD MIGRATION:\n",
        "[ ] Deploy dynamic_reporting_databricks.sql\n",
        "[ ] Test parameter handling\n",
        "[ ] Deploy window_functions_analysis_databricks.sql\n",
        "[ ] Validate window function results\n",
        "[ ] Deploy customer_profitability_databricks.sql\n",
        "[ ] Test PIVOT operation conversion\n",
        "\n",
        "POST-DEPLOYMENT:\n",
        "[ ] Run comprehensive validation tests\n",
        "[ ] Performance benchmarking\n",
        "[ ] User acceptance testing\n",
        "[ ] Document lessons learned\n",
        "[ ] Prepare for Module 3: Data Reconciliation\n",
        "\n",
        "ROLLBACK PLAN:\n",
        "[ ] Delta Lake time travel commands documented\n",
        "[ ] Original SQL Server queries preserved\n",
        "[ ] Rollback procedures tested\n",
        "\n",
        "ESTIMATED EFFORT: {focus_df['hours'].sum()} hours\n",
        "TEAM SIZE: 2-3 developers\n",
        "TIMELINE: 2-3 weeks\n",
        "\n",
        "RISKS & MITIGATION:\n",
        "- Transaction handling complexity ‚Üí Thorough testing with Delta Lake\n",
        "- Variable scope differences ‚Üí Use stored procedures where needed\n",
        "- Performance variations ‚Üí Benchmark and optimize\n",
        "\n",
        "SUCCESS CRITERIA:\n",
        "‚úì All transpiled queries execute without syntax errors\n",
        "‚úì Results match original SQL Server output (within tolerance)\n",
        "‚úì Performance meets or exceeds baseline requirements\n",
        "‚úì Business logic validation passes\n",
        "‚úì Ready for production deployment\n",
        "\"\"\"\n",
        "\n",
        "checklist_path = Path('deployment_checklist.txt')\n",
        "with open(checklist_path, 'w') as f:\n",
        "    f.write(checklist_content)\n",
        "\n",
        "print(f\"‚úÖ Deployment checklist created: {checklist_path}\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\nüéâ MODULE 2 TRANSPILATION COMPLETE!\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìÅ Generated Files:\")\n",
        "print(f\"  ‚Ä¢ Transpiled SQL: ./transpiled_sql/\")\n",
        "print(f\"  ‚Ä¢ Deployment Summary: {summary_path}\")\n",
        "print(f\"  ‚Ä¢ README: {readme_path}\")\n",
        "print(f\"  ‚Ä¢ Checklist: {checklist_path}\")\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"  ‚Ä¢ Target Files: {len(FOCUS_FILES)}\")\n",
        "print(f\"  ‚Ä¢ Focus Scope: Simple to Medium complexity (4.5-7.8)\")\n",
        "print(f\"  ‚Ä¢ Estimated Effort: {focus_df['hours'].sum()} hours\")\n",
        "print(f\"  ‚Ä¢ Cost Avoidance: ${72 * 150:,} (by deferring complex files)\")\n",
        "print(f\"\\nüöÄ Ready for Module 3: Data Reconciliation!\")\n",
        "print(f\"\\nüí° Advanced Challenge: When ready, tackle the 3 Wave 3 files:\")\n",
        "for filename, info in ADVANCED_FILES.items():\n",
        "    print(f\"  ‚Ä¢ {filename} (Complexity: {info['complexity']}/10, {info['hours']}h)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Module Summary & Next Steps\n",
        "\n",
        "### ‚úÖ What We Accomplished\n",
        "\n",
        "1. **Strategic Focus**: Successfully transpiled 5 highest-priority SQL files (4.5-7.8 complexity)\n",
        "2. **Risk Management**: Avoided high-complexity files to ensure manageable scope\n",
        "3. **Pattern Recognition**: Identified key conversion patterns for future use\n",
        "4. **Practical Skills**: Gained hands-on experience with SQL transpilation tools\n",
        "5. **Business Alignment**: Connected technical work to Module 1's business case\n",
        "\n",
        "### üìà Business Value Delivered\n",
        "\n",
        "- **Effort Optimization**: 48 hours (focused scope) vs 120 hours (all files)\n",
        "- **Cost Savings**: $10,800 by deferring complex components\n",
        "- **Risk Reduction**: Started with proven patterns before advanced challenges\n",
        "- **Team Confidence**: Built skills foundation for future complex migrations\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "1. **Immediate**: Run validation tests using `02_validation_tests.sql`\n",
        "2. **Short-term**: Deploy Wave 1 files to development environment\n",
        "3. **Medium-term**: Complete Module 3 (Data Reconciliation) for thorough testing\n",
        "4. **Advanced**: Tackle Wave 3 complex files when team is ready\n",
        "\n",
        "### üéì Key Learnings\n",
        "\n",
        "- **Automated vs Manual**: Understand when each approach is appropriate\n",
        "- **Pattern Recognition**: Common conversion patterns apply across similar SQL\n",
        "- **Risk Assessment**: Technical complexity directly correlates with business risk\n",
        "- **Incremental Approach**: Phased migration reduces overall project risk\n",
        "\n",
        "**Ready for Module 3: Data Reconciliation** üéâ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}