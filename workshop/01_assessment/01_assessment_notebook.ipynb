{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GlobalSupply Corp - Migration Assessment Analysis\n",
    "\n",
    "## 🏢 Business Context\n",
    "**GlobalSupply Corp** operates a complex supply chain data warehouse built on SQL Server with TPC-H style schemas. This workshop demonstrates how to modernize this legacy system to Databricks using Lakebridge for:\n",
    "\n",
    "- **AI-powered supply chain optimization**\n",
    "- **Natural language queries for business users**  \n",
    "- **Real-time analytics and forecasting**\n",
    "- **Scalable cloud-native architecture**\n",
    "\n",
    "## 📊 What This Notebook Covers\n",
    "- **Migration Complexity Analysis** - Which components require the most effort\n",
    "- **Dependency Mapping** - Critical interdependencies affecting migration sequencing\n",
    "- **Risk Assessment** - Potential challenges and mitigation strategies\n",
    "- **Business Value Analysis** - ROI projections for Databricks migration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Prerequisites & Setup\n",
    "\n",
    "### Step 1: Install Required Python Dependencies\n",
    "```bash\n",
    "# Install essential packages for analysis and Excel support\n",
    "pip install pandas matplotlib seaborn numpy openpyxl\n",
    "```\n",
    "\n",
    "**Note:** `openpyxl` is required to read Excel files generated by Lakebridge Analyzer.\n",
    "\n",
    "### Step 2: Install Lakebridge (Optional - for real assessments)\n",
    "```bash\n",
    "# Install Databricks Labs Lakebridge\n",
    "databricks labs install lakebridge\n",
    "\n",
    "# Verify installation\n",
    "databricks labs lakebridge analyze --help\n",
    "```\n",
    "\n",
    "### Step 3: Prepare Legacy SQL Files\n",
    "For real assessments, export your SQL Server workloads to a directory structure like:\n",
    "```\n",
    "legacy_sql/\n",
    "├── analytics/\n",
    "│   ├── supply_chain_performance.sql\n",
    "│   ├── inventory_optimization.sql\n",
    "│   └── customer_profitability.sql\n",
    "├── reports/\n",
    "│   ├── financial_summary.sql\n",
    "│   └── operational_reports.sql\n",
    "└── etl/\n",
    "    ├── data_processing.sql\n",
    "    └── transformations.sql\n",
    "```\n",
    "\n",
    "### Step 4: Run Assessment (with sample data or real SQL files)\n",
    "```bash\n",
    "# Option 1: Use sample SQL files for demonstration\n",
    "python 01_assessment_analyzer.py --generate-samples\n",
    "\n",
    "# Option 2: Assess real SQL files\n",
    "python 01_assessment_analyzer.py --source-directory /path/to/legacy_sql\n",
    "\n",
    "# Option 3: Use Lakebridge directly (if installed and configured)\n",
    "databricks labs lakebridge analyze \\\n",
    "  --source-directory /path/to/legacy_sql \\\n",
    "  --report-file globalsupply_assessment \\\n",
    "  --source-tech mssql\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Understanding Lakebridge Analyzer Output\n",
    "\n",
    "The Lakebridge Analyzer generates comprehensive insights including:\n",
    "\n",
    "### 🔍 Analysis Insights\n",
    "- **Job Complexity Assessment** - Complexity scores (1-10) for each SQL component\n",
    "- **Comprehensive Job Inventory** - All mappings, transformations, functions cataloged\n",
    "- **Cross-System Interdependency Mapping** - Shows how components interact\n",
    "- **Migration Effort Estimates** - Engineering hours required per component\n",
    "\n",
    "### 📊 Key Outputs\n",
    "- **Excel Report** with multiple worksheets:\n",
    "  - Complexity Analysis\n",
    "  - Dependency Mapping  \n",
    "  - Function Usage Statistics\n",
    "  - Migration Estimates\n",
    "  - Risk Assessment\n",
    "\n",
    "### 🎯 Business Value\n",
    "- **Risk Assessment** for migration planning\n",
    "- **Resource Planning** for modernization project  \n",
    "- **Sequencing Guidance** to minimize disruption\n",
    "- **TCO Analysis** for Databricks migration ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for assessment analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Union\n",
    "import logging\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting style for professional reports\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "print(\"📊 Ready for GlobalSupply Corp assessment analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 Intelligent Report Loading System\n",
    "\n",
    "This enhanced loading system can handle:\n",
    "- **Real Lakebridge Assessment Reports** - Automatically detects and normalizes various Excel formats\n",
    "- **Sample Workshop Data** - Falls back to demonstration data when no real report exists\n",
    "- **Column Mapping** - Intelligently maps different column names to standard format\n",
    "- **Error Recovery** - Provides clear guidance when issues occur\n",
    "\n",
    "### Expected Excel Worksheets:\n",
    "- **Summary** - High-level overview and metrics\n",
    "- **Complexity_Analysis** - Detailed complexity scores and effort estimates\n",
    "- **Dependencies** - Component interdependencies and relationships\n",
    "- **Function_Usage** - SQL function usage statistics and compatibility\n",
    "- **Migration_Waves** - Recommended migration sequencing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_assessment_report() -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Intelligent assessment report discovery with multiple fallback strategies\n",
    "    \"\"\"\n",
    "    current_dir = Path('.')\n",
    "    \n",
    "    # Look for assessment reports with comprehensive naming patterns\n",
    "    patterns = [\n",
    "        '*assessment*.xlsx', '*globalsupply*.xlsx', '*remorph*.xlsx',\n",
    "        '*lakebridge*.xlsx', '*migration*.xlsx', '*analysis*.xlsx'\n",
    "    ]\n",
    "    \n",
    "    excel_files = []\n",
    "    for pattern in patterns:\n",
    "        excel_files.extend(list(current_dir.glob(pattern)))\n",
    "    \n",
    "    if not excel_files:\n",
    "        # Look for any Excel files as fallback\n",
    "        excel_files = list(current_dir.glob('*.xlsx'))\n",
    "    \n",
    "    if excel_files:\n",
    "        # Return the most recent file\n",
    "        latest_file = max(excel_files, key=lambda x: x.stat().st_mtime)\n",
    "        return latest_file\n",
    "    \n",
    "    return None\n",
    "\n",
    "def normalize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize column names to standard format for consistent processing\n",
    "    \"\"\"\n",
    "    # Standardize column names (remove spaces, lowercase, handle variations)\n",
    "    df.columns = [col.strip().replace(' ', '_').replace('-', '_').lower() for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def map_columns_to_standard(df: pd.DataFrame, sheet_type: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map various column name variations to standardized names\n",
    "    \"\"\"\n",
    "    if sheet_type == 'Complexity_Analysis':\n",
    "        column_mappings = {\n",
    "            'filename': 'file_name', 'file': 'file_name', 'script_name': 'file_name',\n",
    "            'name': 'file_name', 'object_name': 'file_name',\n",
    "            'loc': 'lines_of_code', 'lines': 'lines_of_code', 'line_count': 'lines_of_code',\n",
    "            'complexity': 'complexity_score', 'score': 'complexity_score',\n",
    "            'effort': 'migration_hours', 'hours': 'migration_hours', \n",
    "            'migration_effort': 'migration_hours', 'work_hours': 'migration_hours',\n",
    "            'risk': 'risk_level', 'risk_category': 'risk_level',\n",
    "            'functions': 'functions_used', 'function_count': 'functions_used',\n",
    "            'tables': 'table_references', 'table_count': 'table_references'\n",
    "        }\n",
    "    elif sheet_type == 'Dependencies':\n",
    "        column_mappings = {\n",
    "            'source': 'source_object', 'from_object': 'source_object',\n",
    "            'target': 'target_object', 'to_object': 'target_object',\n",
    "            'type': 'dependency_type', 'dep_type': 'dependency_type',\n",
    "            'priority': 'criticality', 'importance': 'criticality'\n",
    "        }\n",
    "    elif sheet_type == 'Function_Usage':\n",
    "        column_mappings = {\n",
    "            'function': 'function_name', 'func_name': 'function_name',\n",
    "            'count': 'usage_count', 'usage': 'usage_count',\n",
    "            'complexity_score': 'complexity_impact', 'impact': 'complexity_impact',\n",
    "            'compatibility': 'databricks_compatibility', 'compat': 'databricks_compatibility'\n",
    "        }\n",
    "    else:\n",
    "        column_mappings = {}\n",
    "    \n",
    "    # Apply mappings\n",
    "    for old_col, new_col in column_mappings.items():\n",
    "        if old_col in df.columns and new_col not in df.columns:\n",
    "            df[new_col] = df[old_col]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ensure_required_columns(df: pd.DataFrame, sheet_type: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure all required columns exist with sensible defaults\n",
    "    \"\"\"\n",
    "    if sheet_type == 'Complexity_Analysis':\n",
    "        required_columns = {\n",
    "            'file_name': 'Unknown',\n",
    "            'complexity_score': 5.0,\n",
    "            'migration_hours': 8,\n",
    "            'risk_level': 'Medium',\n",
    "            'lines_of_code': 100,\n",
    "            'category': 'Other',\n",
    "            'sql_features': 'Standard SQL'\n",
    "        }\n",
    "    elif sheet_type == 'Dependencies':\n",
    "        required_columns = {\n",
    "            'source_object': 'Unknown',\n",
    "            'target_object': 'Unknown', \n",
    "            'dependency_type': 'Unknown',\n",
    "            'criticality': 'Medium'\n",
    "        }\n",
    "    elif sheet_type == 'Function_Usage':\n",
    "        required_columns = {\n",
    "            'function_name': 'Unknown',\n",
    "            'usage_count': 1,\n",
    "            'complexity_impact': 3,\n",
    "            'databricks_compatibility': 'Unknown'\n",
    "        }\n",
    "    else:\n",
    "        required_columns = {}\n",
    "    \n",
    "    # Add missing columns with defaults\n",
    "    for col, default_val in required_columns.items():\n",
    "        if col not in df.columns:\n",
    "            df[col] = default_val\n",
    "            print(f\"➕ Added missing column '{col}' with default value\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_sample_data() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create comprehensive sample data for workshop demonstration\n",
    "    \"\"\"\n",
    "    print(\"🎭 Creating comprehensive sample GlobalSupply Corp assessment data...\")\n",
    "    \n",
    "    # Sample complexity analysis representing real-world SQL Server workloads\n",
    "    complexity_data = pd.DataFrame({\n",
    "        'file_name': [\n",
    "            'supply_chain_performance.sql', 'inventory_optimization.sql', 'customer_profitability.sql',\n",
    "            'supplier_risk_assessment.sql', 'dynamic_reporting.sql', 'window_functions_analysis.sql',\n",
    "            'financial_summary.sql', 'order_processing.sql'\n",
    "        ],\n",
    "        'lines_of_code': [145, 198, 167, 223, 89, 134, 67, 89],\n",
    "        'complexity_score': [8.5, 9.2, 7.8, 9.8, 6.5, 7.2, 4.5, 5.1],\n",
    "        'functions_used': [12, 18, 14, 22, 8, 16, 6, 9],\n",
    "        'table_references': [8, 6, 7, 9, 5, 4, 3, 4],\n",
    "        'migration_hours': [16, 24, 18, 32, 8, 12, 4, 6],\n",
    "        'risk_level': ['Medium', 'High', 'Medium', 'High', 'Low', 'Medium', 'Low', 'Low'],\n",
    "        'category': ['Analytics', 'Analytics', 'Analytics', 'Analytics', 'Reporting', 'Analytics', 'Reporting', 'OLTP'],\n",
    "        'sql_features': [\n",
    "            'CTEs, Window Functions, Complex Joins',\n",
    "            'Recursive CTEs, PIVOT, Advanced Analytics', \n",
    "            'PIVOT, Window Functions, String Aggregation',\n",
    "            'Recursive CTEs, Dynamic SQL, Risk Scoring',\n",
    "            'Dynamic SQL, Conditional Logic',\n",
    "            'Advanced Window Functions, LAG/LEAD',\n",
    "            'Basic Aggregation, Simple Joins',\n",
    "            'CRUD Operations, Transactions'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Sample dependency mapping with realistic relationships\n",
    "    dependency_data = pd.DataFrame({\n",
    "        'source_object': [\n",
    "            'customer_profitability.sql', 'supplier_risk_assessment.sql', 'inventory_optimization.sql',\n",
    "            'supply_chain_performance.sql', 'window_functions_analysis.sql', 'order_processing.sql'\n",
    "        ],\n",
    "        'target_object': [\n",
    "            'financial_summary.sql', 'supply_chain_performance.sql', 'order_processing.sql',\n",
    "            'dynamic_reporting.sql', 'customer_profitability.sql', 'financial_summary.sql'\n",
    "        ],\n",
    "        'dependency_type': ['View', 'Procedure', 'Table', 'View', 'Function', 'Table'],\n",
    "        'criticality': ['High', 'High', 'Medium', 'Medium', 'Low', 'Medium']\n",
    "    })\n",
    "    \n",
    "    # Sample function usage with Databricks compatibility assessment\n",
    "    function_data = pd.DataFrame({\n",
    "        'function_name': [\n",
    "            'ROW_NUMBER', 'RANK', 'LAG', 'LEAD', 'SUM', 'AVG', 'COUNT', 'DATEDIFF', \n",
    "            'DATEADD', 'STRING_AGG', 'PIVOT', 'CASE', 'CTE', 'RECURSIVE_CTE', 'STDEV', 'VAR'\n",
    "        ],\n",
    "        'usage_count': [15, 12, 8, 6, 25, 20, 30, 18, 14, 5, 3, 22, 18, 2, 4, 3],\n",
    "        'complexity_impact': [3, 3, 4, 4, 1, 1, 1, 2, 2, 4, 5, 2, 3, 5, 3, 3],\n",
    "        'databricks_compatibility': [\n",
    "            'Direct', 'Direct', 'Direct', 'Direct', 'Direct', 'Direct', 'Direct', 'Modified',\n",
    "            'Modified', 'Modified', 'Modified', 'Direct', 'Direct', 'Complex', 'Direct', 'Direct'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        'Complexity_Analysis': complexity_data,\n",
    "        'Dependencies': dependency_data,\n",
    "        'Function_Usage': function_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced assessment report loading with intelligent fallback\n",
    "def load_assessment_data() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Master function to load assessment data from various sources\n",
    "    \"\"\"\n",
    "    sheets_data = {}\n",
    "    \n",
    "    # Step 1: Try to find existing assessment report\n",
    "    report_file = find_assessment_report()\n",
    "    \n",
    "    if report_file:\n",
    "        print(f\"📄 Found assessment report: {report_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the Excel file and examine structure\n",
    "            excel_data = pd.ExcelFile(report_file)\n",
    "            print(f\"📋 Available worksheets: {excel_data.sheet_names}\")\n",
    "            \n",
    "            # Map common sheet name variations to standard names\n",
    "            sheet_mappings = {\n",
    "                'Summary': ['Summary', 'Overview', 'Report_Summary'],\n",
    "                'Complexity_Analysis': ['Complexity', 'Analysis', 'Complexity_Analysis', 'Job_Analysis', 'Complexity Analysis'],\n",
    "                'Dependencies': ['Dependencies', 'Dependency', 'Relationships', 'Job_Dependencies'],\n",
    "                'Function_Usage': ['Functions', 'Function_Usage', 'SQL_Functions', 'Features', 'Function Usage'],\n",
    "                'Migration_Waves': ['Waves', 'Migration_Waves', 'Migration_Strategy', 'Migration Waves']\n",
    "            }\n",
    "            \n",
    "            # Load and normalize each sheet\n",
    "            for standard_name, possible_names in sheet_mappings.items():\n",
    "                sheet_found = False\n",
    "                for possible_name in possible_names:\n",
    "                    if possible_name in excel_data.sheet_names:\n",
    "                        try:\n",
    "                            df = pd.read_excel(report_file, sheet_name=possible_name)\n",
    "                            if len(df) > 0:  # Only process non-empty sheets\n",
    "                                # Apply all normalization steps\n",
    "                                df = normalize_column_names(df)\n",
    "                                df = map_columns_to_standard(df, standard_name)\n",
    "                                df = ensure_required_columns(df, standard_name)\n",
    "                                \n",
    "                                sheets_data[standard_name] = df\n",
    "                                print(f\"✅ Loaded '{possible_name}' as '{standard_name}': {len(df)} rows, {len(df.columns)} columns\")\n",
    "                                sheet_found = True\n",
    "                                break\n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Could not load sheet '{possible_name}': {e}\")\n",
    "                \n",
    "                if not sheet_found:\n",
    "                    print(f\"⚠️ No matching sheet found for '{standard_name}'\")\n",
    "            \n",
    "            # If we got at least some data, we're good\n",
    "            if sheets_data:\n",
    "                print(f\"✅ Successfully loaded real assessment report with {len(sheets_data)} worksheets\")\n",
    "                return sheets_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading Excel file: {e}\")\n",
    "    \n",
    "    # Step 2: Fallback to sample data\n",
    "    print(\"📊 No usable assessment report found, using sample workshop data\")\n",
    "    print(\"\")\n",
    "    print(\"💡 To use a real assessment report:\")\n",
    "    print(\"   1. Ensure Lakebridge is installed: databricks labs install lakebridge\")\n",
    "    print(\"   2. Run assessment: python 01_assessment_analyzer.py --generate-samples\")\n",
    "    print(\"   3. Or place your existing .xlsx report in this directory\")\n",
    "    print(\"\")\n",
    "    \n",
    "    return create_sample_data()\n",
    "\n",
    "# Load the assessment data using our intelligent system\n",
    "sheets_data = load_assessment_data()\n",
    "\n",
    "# Display summary of loaded data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 ASSESSMENT DATA LOADING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for sheet_name, df in sheets_data.items():\n",
    "    print(f\"📋 {sheet_name}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    if len(df.columns) > 0:\n",
    "        print(f\"   Columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🎯 Data loading complete! Ready for analysis and visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Assessment Overview Dashboard\n",
    "\n",
    "This section provides a high-level overview of the migration assessment, showing key metrics that executives and project managers need to understand the scope and complexity of the modernization effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive assessment overview with robust error handling\n",
    "if 'Complexity_Analysis' in sheets_data and len(sheets_data['Complexity_Analysis']) > 0:\n",
    "    df = sheets_data['Complexity_Analysis']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 GLOBALSUPPLY CORP - MIGRATION ASSESSMENT OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Key metrics with safe calculations\n",
    "    total_files = len(df)\n",
    "    total_loc = df['lines_of_code'].sum() if 'lines_of_code' in df.columns else 0\n",
    "    total_hours = df['migration_hours'].sum() if 'migration_hours' in df.columns else 0\n",
    "    avg_complexity = df['complexity_score'].mean() if 'complexity_score' in df.columns else 5.0\n",
    "    \n",
    "    print(f\"📁 Total SQL Components: {total_files}\")\n",
    "    print(f\"📏 Total Lines of Code: {total_loc:,}\")\n",
    "    print(f\"⏱️  Estimated Migration Hours: {total_hours}\")\n",
    "    print(f\"📈 Average Complexity Score: {avg_complexity:.1f}/10\")\n",
    "    print(f\"💰 Estimated Cost (@$150/hour): ${total_hours * 150:,}\")\n",
    "    \n",
    "    # Risk distribution analysis with error handling\n",
    "    if 'risk_level' in df.columns:\n",
    "        risk_distribution = df['risk_level'].value_counts()\n",
    "        print(f\"\\n🚦 RISK DISTRIBUTION:\")\n",
    "        for risk, count in risk_distribution.items():\n",
    "            percentage = (count/len(df)*100)\n",
    "            risk_emoji = {'Low': '🟢', 'Medium': '🟡', 'High': '🔴'}\n",
    "            print(f\"   {risk_emoji.get(risk, '⚪')} {risk}: {count} files ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Category distribution with error handling\n",
    "    if 'category' in df.columns:\n",
    "        category_distribution = df['category'].value_counts()\n",
    "        print(f\"\\n📊 WORKLOAD CATEGORIES:\")\n",
    "        for category, count in category_distribution.items():\n",
    "            percentage = (count/len(df)*100)\n",
    "            print(f\"   📈 {category}: {count} files ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Timeline estimates\n",
    "    weeks_single = total_hours / 40 if total_hours > 0 else 0\n",
    "    weeks_team = total_hours / 160 if total_hours > 0 else 0  # 4-person team\n",
    "    \n",
    "    print(f\"\\n📅 TIMELINE ESTIMATES:\")\n",
    "    print(f\"   👤 Single Developer: {weeks_single:.1f} weeks\")\n",
    "    print(f\"   👥 4-Person Team: {weeks_team:.1f} weeks\")\n",
    "    \n",
    "    print(\"\\n💡 KEY INSIGHTS:\")\n",
    "    high_risk_count = len(df[df['risk_level'] == 'High']) if 'risk_level' in df.columns else 0\n",
    "    complex_count = len(df[df['complexity_score'] > 8]) if 'complexity_score' in df.columns else 0\n",
    "    \n",
    "    if high_risk_count > 0:\n",
    "        print(f\"   ⚠️  {high_risk_count} high-risk components need expert attention\")\n",
    "    if complex_count > 0:\n",
    "        print(f\"   🧠 {complex_count} highly complex components (>8/10 complexity)\")\n",
    "    print(f\"   🎯 Recommended phased approach in 3 migration waves\")\n",
    "    print(f\"   📈 Expected 3-5x performance improvement with Databricks\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠️ Complexity analysis data not available\")\n",
    "    print(\"💡 This could happen if:\")\n",
    "    print(\"   • The Excel report doesn't have a recognized complexity worksheet\")\n",
    "    print(\"   • The worksheet is empty or has incorrect column names\")\n",
    "    print(\"   • Try regenerating the assessment report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Complexity Analysis Visualizations\n",
    "\n",
    "These visualizations help identify which components will require the most attention during migration and provide insights into the overall complexity distribution of the SQL codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust complexity analysis with comprehensive error handling\n",
    "if 'Complexity_Analysis' in sheets_data and len(sheets_data['Complexity_Analysis']) > 0:\n",
    "    df = sheets_data['Complexity_Analysis']\n",
    "    \n",
    "    # Validate required columns exist\n",
    "    required_cols = ['complexity_score', 'migration_hours']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"⚠️ Missing required columns: {missing_cols}\")\n",
    "        print(\"Adding default values for visualization...\")\n",
    "        for col in missing_cols:\n",
    "            if col == 'complexity_score':\n",
    "                df[col] = np.random.uniform(4, 9, len(df))\n",
    "            elif col == 'migration_hours':\n",
    "                df[col] = np.random.randint(4, 32, len(df))\n",
    "    \n",
    "    try:\n",
    "        # Create comprehensive complexity dashboard\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('GlobalSupply Corp - Migration Complexity Analysis Dashboard', \n",
    "                     fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # 1. Complexity Score Distribution\n",
    "        axes[0, 0].hist(df['complexity_score'], bins=10, alpha=0.7, color='skyblue', \n",
    "                        edgecolor='black', linewidth=1.2)\n",
    "        axes[0, 0].set_title('📊 Complexity Score Distribution', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Complexity Score (1-10 scale)')\n",
    "        axes[0, 0].set_ylabel('Number of SQL Components')\n",
    "        \n",
    "        # Add mean line and statistics\n",
    "        mean_complexity = df['complexity_score'].mean()\n",
    "        axes[0, 0].axvline(mean_complexity, color='red', linestyle='--', linewidth=2,\n",
    "                           label=f'Mean: {mean_complexity:.1f}')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # 2. Risk Level Distribution (if available)\n",
    "        if 'risk_level' in df.columns:\n",
    "            risk_counts = df['risk_level'].value_counts()\n",
    "            colors = {'Low': '#2ecc71', 'Medium': '#f39c12', 'High': '#e74c3c'}\n",
    "            risk_colors = [colors.get(risk, '#95a5a6') for risk in risk_counts.index]\n",
    "            \n",
    "            wedges, texts, autotexts = axes[0, 1].pie(risk_counts.values, labels=risk_counts.index, \n",
    "                                                      autopct='%1.1f%%', colors=risk_colors, \n",
    "                                                      startangle=90, explode=(0.05, 0.05, 0.05))\n",
    "            axes[0, 1].set_title('🚦 Migration Risk Distribution', fontweight='bold')\n",
    "            \n",
    "            # Enhance pie chart text\n",
    "            for autotext in autotexts:\n",
    "                autotext.set_color('white')\n",
    "                autotext.set_fontweight('bold')\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'Risk Level Data\\nNot Available', \n",
    "                           transform=axes[0, 1].transAxes, ha='center', va='center',\n",
    "                           fontsize=14, bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "            axes[0, 1].set_title('🚦 Migration Risk Distribution', fontweight='bold')\n",
    "        \n",
    "        # 3. Effort vs Complexity Scatter Plot\n",
    "        bubble_size = df.get('lines_of_code', pd.Series([100] * len(df)))\n",
    "        scatter = axes[1, 0].scatter(df['complexity_score'], df['migration_hours'], \n",
    "                                    c=bubble_size, cmap='viridis', alpha=0.8, \n",
    "                                    s=150, edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Complexity Score')\n",
    "        axes[1, 0].set_ylabel('Migration Hours')\n",
    "        axes[1, 0].set_title('⚡ Effort vs Complexity (bubble size = Lines of Code)', fontweight='bold')\n",
    "        \n",
    "        # Add colorbar if we have size data\n",
    "        if 'lines_of_code' in df.columns:\n",
    "            cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "            cbar.set_label('Lines of Code', rotation=270, labelpad=20)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(df['complexity_score'], df['migration_hours'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[1, 0].plot(df['complexity_score'], p(df['complexity_score']), \"r--\", alpha=0.7)\n",
    "        \n",
    "        # 4. Category-wise Migration Hours (if available)\n",
    "        if 'category' in df.columns:\n",
    "            category_hours = df.groupby('category')['migration_hours'].sum().sort_values(ascending=True)\n",
    "            bars = axes[1, 1].barh(category_hours.index, category_hours.values, \n",
    "                                   color=['#3498db', '#e67e22', '#9b59b6'], alpha=0.8)\n",
    "            \n",
    "            axes[1, 1].set_xlabel('Total Migration Hours')\n",
    "            axes[1, 1].set_title('📊 Migration Effort by Category', fontweight='bold')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for i, (bar, value) in enumerate(zip(bars, category_hours.values)):\n",
    "                axes[1, 1].text(value + 0.5, i, f'{int(value)}h', \n",
    "                                 va='center', fontweight='bold')\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'Category Data\\nNot Available', \n",
    "                           transform=axes[1, 1].transAxes, ha='center', va='center',\n",
    "                           fontsize=14, bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "            axes[1, 1].set_title('📊 Migration Effort by Category', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.93)\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional insights table with safe column access\n",
    "        print(\"\\n📋 DETAILED COMPLEXITY BREAKDOWN:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Show top 5 most complex components\n",
    "        display_columns = ['file_name', 'complexity_score', 'migration_hours']\n",
    "        if 'sql_features' in df.columns:\n",
    "            display_columns.append('sql_features')\n",
    "        \n",
    "        available_columns = [col for col in display_columns if col in df.columns]\n",
    "        \n",
    "        if available_columns:\n",
    "            top_complex = df.nlargest(5, 'complexity_score')[available_columns]\n",
    "            \n",
    "            for _, row in top_complex.iterrows():\n",
    "                print(f\"📄 {row.get('file_name', 'Unknown')}:\")\n",
    "                print(f\"   🎯 Complexity: {row.get('complexity_score', 'N/A')}/10\")\n",
    "                print(f\"   ⏱️  Effort: {row.get('migration_hours', 'N/A')} hours\")\n",
    "                if 'sql_features' in row:\n",
    "                    print(f\"   🔧 Features: {row.get('sql_features', 'N/A')}\")\n",
    "                print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating visualizations: {e}\")\n",
    "        print(\"This might be due to data format issues. The analysis can still continue.\")\n",
    "    \nelse:\n",
    "    print(\"⚠️ Complexity analysis data not available for visualization\")\n",
    "    print(\"💡 Make sure your assessment report includes a 'Complexity_Analysis' worksheet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 Dependency Analysis\n",
    "\n",
    "Understanding dependencies is crucial for migration sequencing. This analysis shows which components depend on others and helps plan the migration order to minimize risk and ensure system stability throughout the process.\n",
    "\n",
    "### Why Dependencies Matter:\n",
    "- **Migration Sequencing** - Dependencies must be migrated before dependent components\n",
    "- **Risk Management** - High-criticality dependencies require extra attention\n",
    "- **Testing Strategy** - Dependent components need integrated testing\n",
    "- **Rollback Planning** - Understanding dependencies helps plan safe rollback procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust dependency analysis with comprehensive error handling\n",
    "if 'Dependencies' in sheets_data and len(sheets_data['Dependencies']) > 0:\n",
    "    dep_df = sheets_data['Dependencies']\n",
    "    \n",
    "    print(\"🔗 DEPENDENCY ANALYSIS FOR MIGRATION PLANNING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        print(f\"📊 Total Dependencies Identified: {len(dep_df)}\")\n",
    "        \n",
    "        # Analyze dependency criticality (if available)\n",
    "        if 'criticality' in dep_df.columns:\n",
    "            criticality_counts = dep_df['criticality'].value_counts()\n",
    "            print(\"\\n🚦 Dependency Criticality Breakdown:\")\n",
    "            for crit, count in criticality_counts.items():\n",
    "                crit_emoji = {'High': '🔴', 'Medium': '🟡', 'Low': '🟢'}\n",
    "                percentage = (count / len(dep_df) * 100)\n",
    "                print(f\"   {crit_emoji.get(crit, '⚪')} {crit}: {count} dependencies ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Analyze dependency types (if available)\n",
    "        if 'dependency_type' in dep_df.columns:\n",
    "            type_counts = dep_df['dependency_type'].value_counts()\n",
    "            print(\"\\n📋 Dependency Types:\")\n",
    "            for dep_type, count in type_counts.items():\n",
    "                print(f\"   📊 {dep_type}: {count} dependencies\")\n",
    "        \n",
    "        print(\"\\n🔗 Critical Dependency Chains:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Show high-criticality dependencies first (if available)\n",
    "        if 'criticality' in dep_df.columns:\n",
    "            high_crit_deps = dep_df[dep_df['criticality'] == 'High']\n",
    "        else:\n",
    "            high_crit_deps = dep_df.head(5)  # Show first 5 as fallback\n",
    "        \n",
    "        for _, row in high_crit_deps.iterrows():\n",
    "            source = str(row.get('source_object', 'Unknown')).replace('.sql', '')\n",
    "            target = str(row.get('target_object', 'Unknown')).replace('.sql', '')\n",
    "            dep_type = row.get('dependency_type', 'Unknown')\n",
    "            criticality = row.get('criticality', 'Unknown')\n",
    "            \n",
    "            print(f\"   {source} ➜ {target}\")\n",
    "            print(f\"     Type: {dep_type} | Criticality: {criticality}\")\n",
    "            print()\n",
    "        \n",
    "        # Migration sequencing recommendations\n",
    "        print(\"📋 MIGRATION SEQUENCING RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Find components with no dependencies (good starting points)\n",
    "        if 'source_object' in dep_df.columns and 'target_object' in dep_df.columns:\n",
    "            all_sources = set(dep_df['source_object'])\n",
    "            all_targets = set(dep_df['target_object'])\n",
    "            independent_components = all_sources - all_targets\n",
    "            \n",
    "            if independent_components:\n",
    "                print(\"🏁 Start with these independent components:\")\n",
    "                for comp in list(independent_components)[:3]:\n",
    "                    print(f\"   ✅ {str(comp).replace('.sql', '')}\")\n",
    "            \n",
    "            # Find components that many others depend on\n",
    "            dependency_counts = dep_df['source_object'].value_counts()\n",
    "            if len(dependency_counts) > 0:\n",
    "                print(\"\\n🎯 High-priority components (many dependencies):\")\n",
    "                for comp, count in dependency_counts.head(3).items():\n",
    "                    print(f\"   🔴 {str(comp).replace('.sql', '')} ({count} dependent components)\")\n",
    "        \n",
    "        # Create dependency visualizations\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle('GlobalSupply Corp - Dependency Analysis', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 1. Criticality distribution (if available)\n",
    "        if 'criticality' in dep_df.columns:\n",
    "            criticality_counts = dep_df['criticality'].value_counts()\n",
    "            colors = {'High': '#e74c3c', 'Medium': '#f39c12', 'Low': '#2ecc71'}\n",
    "            crit_colors = [colors.get(crit, '#95a5a6') for crit in criticality_counts.index]\n",
    "            \n",
    "            wedges, texts, autotexts = ax1.pie(criticality_counts.values, \n",
    "                                               labels=criticality_counts.index,\n",
    "                                               autopct='%1.1f%%', colors=crit_colors,\n",
    "                                               startangle=90, explode=(0.05, 0.05, 0.05))\n",
    "            ax1.set_title('🚦 Dependency Criticality', fontweight='bold')\n",
    "            \n",
    "            for autotext in autotexts:\n",
    "                autotext.set_color('white')\n",
    "                autotext.set_fontweight('bold')\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, 'Criticality Data\\nNot Available', \n",
    "                    ha='center', va='center', fontsize=14,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "            ax1.set_title('🚦 Dependency Criticality', fontweight='bold')\n",
    "        \n",
    "        # 2. Dependency type distribution (if available)\n",
    "        if 'dependency_type' in dep_df.columns:\n",
    "            type_counts = dep_df['dependency_type'].value_counts()\n",
    "            bars = ax2.bar(type_counts.index, type_counts.values, \n",
    "                          color=['#3498db', '#e67e22', '#9b59b6', '#1abc9c'][:len(type_counts)])\n",
    "            \n",
    "            ax2.set_title('📊 Dependencies by Type', fontweight='bold')\n",
    "            ax2.set_xlabel('Dependency Type')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, type_counts.values):\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                         str(value), ha='center', va='bottom', fontweight='bold')\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'Dependency Type\\nData Not Available', \n",
    "                    ha='center', va='center', fontsize=14,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "            ax2.set_title('📊 Dependencies by Type', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in dependency analysis: {e}\")\n",
    "        print(\"This might be due to data format issues, but basic analysis completed.\")\n",
    "    \nelse:\n",
    "    print(\"⚠️ Dependency analysis data not available\")\n",
    "    print(\"💡 In a real assessment, Lakebridge would identify:\")\n",
    "    print(\"   • Table-to-table dependencies\")\n",
    "    print(\"   • View-to-table relationships\")\n",
    "    print(\"   • Stored procedure call chains\")\n",
    "    print(\"   • Cross-database dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 SQL Function Analysis\n",
    "\n",
    "This analysis examines SQL functions and features used in the legacy code to understand:\n",
    "- **Databricks Compatibility** - Which functions translate directly vs need modification\n",
    "- **Complexity Drivers** - Functions that contribute most to migration complexity\n",
    "- **Focus Areas** - Where development effort should be concentrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive function analysis with robust error handling\n",
    "if 'Function_Usage' in sheets_data and len(sheets_data['Function_Usage']) > 0:\n",
    "    func_df = sheets_data['Function_Usage']\n",
    "    \n",
    "    print(\"🔧 SQL FUNCTION COMPATIBILITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Overall statistics\n",
    "        total_functions = func_df['usage_count'].sum() if 'usage_count' in func_df.columns else len(func_df)\n",
    "        unique_functions = len(func_df)\n",
    "        \n",
    "        print(f\"📊 Total Function Usages: {total_functions}\")\n",
    "        print(f\"🔧 Unique Functions: {unique_functions}\")\n",
    "        \n",
    "        # Compatibility breakdown (if available)\n",
    "        if 'databricks_compatibility' in func_df.columns:\n",
    "            compat_counts = func_df['databricks_compatibility'].value_counts()\n",
    "            print(\"\\n🎯 DATABRICKS COMPATIBILITY BREAKDOWN:\")\n",
    "            \n",
    "            compat_emojis = {\n",
    "                'Direct': '✅ Direct translation',\n",
    "                'Modified': '🔄 Requires modification', \n",
    "                'Complex': '⚠️ Complex migration',\n",
    "                'Manual': '🛠️ Manual rewrite needed'\n",
    "            }\n",
    "            \n",
    "            for compat, count in compat_counts.items():\n",
    "                percentage = (count / len(func_df) * 100)\n",
    "                emoji_desc = compat_emojis.get(compat, f'🔍 {compat}')\n",
    "                print(f\"   {emoji_desc}: {count} functions ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Top complexity drivers (if available)\n",
    "        if 'complexity_impact' in func_df.columns:\n",
    "            print(\"\\n🎯 TOP COMPLEXITY DRIVERS:\")\n",
    "            complexity_drivers = func_df.nlargest(5, 'complexity_impact')\n",
    "            \n",
    "            for _, row in complexity_drivers.iterrows():\n",
    "                function = row.get('function_name', 'Unknown')\n",
    "                usage = row.get('usage_count', 'N/A')\n",
    "                complexity = row.get('complexity_impact', 'N/A')\n",
    "                compat = row.get('databricks_compatibility', 'Unknown')\n",
    "                \n",
    "                print(f\"   🔧 {function}:\")\n",
    "                print(f\"      Used {usage} times | Complexity: {complexity}/5 | Compatibility: {compat}\")\n",
    "        \n",
    "        # Most used functions (if available)\n",
    "        if 'usage_count' in func_df.columns:\n",
    "            print(\"\\n📈 MOST FREQUENTLY USED FUNCTIONS:\")\n",
    "            top_used = func_df.nlargest(5, 'usage_count')\n",
    "            \n",
    "            for _, row in top_used.iterrows():\n",
    "                function = row.get('function_name', 'Unknown')\n",
    "                usage = row.get('usage_count', 'N/A')\n",
    "                compat = row.get('databricks_compatibility', 'Unknown')\n",
    "                \n",
    "                status_emoji = {'Direct': '✅', 'Modified': '🔄', 'Complex': '⚠️'}.get(compat, '🔍')\n",
    "                print(f\"   {status_emoji} {function}: {usage} usages ({compat})\")\n",
    "        \n",
    "        # Create function analysis visualization\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\n",
    "        fig.suptitle('SQL Function Usage & Compatibility Analysis', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 1. Compatibility distribution (if available)\n",
    "        if 'databricks_compatibility' in func_df.columns:\n",
    "            compat_counts = func_df['databricks_compatibility'].value_counts()\n",
    "            colors = {'Direct': '#2ecc71', 'Modified': '#f39c12', 'Complex': '#e74c3c', 'Manual': '#8e44ad'}\n",
    "            compat_colors = [colors.get(comp, '#95a5a6') for comp in compat_counts.index]\n",
    "            \n",
    "            ax1.pie(compat_counts.values, labels=compat_counts.index, autopct='%1.1f%%',\n",
    "                    colors=compat_colors, startangle=90)\n",
    "            ax1.set_title('🎯 Compatibility Distribution')\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, 'Compatibility Data\\nNot Available', \n",
    "                    ha='center', va='center', fontsize=14,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "            ax1.set_title('🎯 Compatibility Distribution')\n",
    "        \n",
    "        # 2. Top used functions (if available)\n",
    "        if 'usage_count' in func_df.columns and 'function_name' in func_df.columns:\n",
    "            top_used = func_df.nlargest(min(8, len(func_df)), 'usage_count')\n",
    "            bars = ax2.barh(top_used['function_name'], top_used['usage_count'], color='skyblue')\n",
    "            ax2.set_title('📊 Most Used Functions')\n",
    "            ax2.set_xlabel('Usage Count')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, top_used['usage_count']):\n",
    "                ax2.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2,\n",
    "                         str(value), va='center', fontweight='bold')\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'Usage Count Data\\nNot Available', \n",
    "                    ha='center', va='center', fontsize=14,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "            ax2.set_title('📊 Most Used Functions')\n",
    "        \n",
    "        # 3. Complexity vs Usage scatter (if available)\n",
    "        if 'complexity_impact' in func_df.columns and 'usage_count' in func_df.columns:\n",
    "            scatter = ax3.scatter(func_df['usage_count'], func_df['complexity_impact'],\n",
    "                                 alpha=0.7, s=100, color='coral')\n",
    "            ax3.set_xlabel('Usage Count')\n",
    "            ax3.set_ylabel('Complexity Impact (1-5)')\n",
    "            ax3.set_title('⚡ Usage vs Complexity')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'Complexity/Usage\\nData Not Available', \n",
    "                    ha='center', va='center', fontsize=14,\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "            ax3.set_title('⚡ Usage vs Complexity')\n",
    "        \n",
    "        # 4. Key insights\n",
    "        ax4.text(0.1, 0.8, \"🎯 KEY MIGRATION INSIGHTS:\", transform=ax4.transAxes, \n",
    "                 fontsize=12, fontweight='bold')\n",
    "        \n",
    "        insights = [\n",
    "            \"• Focus on high-usage complex functions first\",\n",
    "            \"• Test modified functions thoroughly\", \n",
    "            \"• Consider performance implications\",\n",
    "            \"• Plan for user training on new syntax\"\n",
    "        ]\n",
    "        \n",
    "        for i, insight in enumerate(insights):\n",
    "            ax4.text(0.1, 0.6 - i*0.1, insight, transform=ax4.transAxes, fontsize=10)\n",
    "        \n",
    "        ax4.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in function analysis: {e}\")\n",
    "        print(\"This might be due to data format issues, but basic analysis completed.\")\n",
    "    \nelse:\n",
    "    print(\"⚠️ Function usage data not available\")\n",
    "    print(\"💡 In a real assessment, this would show:\")\n",
    "    print(\"   • SQL Server specific functions used\")\n",
    "    print(\"   • Databricks compatibility status\")\n",
    "    print(\"   • Migration complexity by function type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Migration Planning Dashboard\n",
    "\n",
    "This section provides actionable insights for planning the migration, including:\n",
    "- **Migration Wave Strategy** - Phased approach based on complexity and risk\n",
    "- **Resource Planning** - Team size and timeline estimates\n",
    "- **Risk Mitigation** - Specific recommendations for high-risk components\n",
    "- **Cost-Benefit Analysis** - Investment vs expected returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive migration planning with robust error handling\n",
    "if 'Complexity_Analysis' in sheets_data and len(sheets_data['Complexity_Analysis']) > 0:\n",
    "    df = sheets_data['Complexity_Analysis'].copy()\n",
    "    \n",
    "    print(\"📋 GLOBALSUPPLY CORP - MIGRATION PLANNING STRATEGY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Define migration waves based on complexity and risk\n",
    "        def assign_migration_wave(row):\n",
    "            \"\"\"\n",
    "            Assign components to migration waves based on complexity and risk\n",
    "            Wave 1: Low risk, low-medium complexity (Quick Wins)\n",
    "            Wave 2: Medium risk or higher complexity (Standard Migration)\n",
    "            Wave 3: High risk or very high complexity (Complex Components)\n",
    "            \"\"\"\n",
    "            risk = row.get('risk_level', 'Medium')\n",
    "            complexity = row.get('complexity_score', 5.0)\n",
    "            \n",
    "            if risk == 'Low' and complexity < 6:\n",
    "                return 'Wave 1 - Quick Wins'\n",
    "            elif risk == 'Medium' or (risk == 'Low' and complexity >= 6):\n",
    "                return 'Wave 2 - Standard Migration'\n",
    "            else:\n",
    "                return 'Wave 3 - Complex Components'\n",
    "        \n",
    "        df['Migration_Wave'] = df.apply(assign_migration_wave, axis=1)\n",
    "        \n",
    "        # Analyze migration waves with safe aggregation\n",
    "        wave_analysis = df.groupby('Migration_Wave').agg({\n",
    "            'file_name': 'count',\n",
    "            'migration_hours': lambda x: x.sum() if 'migration_hours' in df.columns else 0, \n",
    "            'complexity_score': lambda x: x.mean() if 'complexity_score' in df.columns else 5.0,\n",
    "            'lines_of_code': lambda x: x.sum() if 'lines_of_code' in df.columns else 0\n",
    "        }).round(1)\n",
    "        \n",
    "        wave_analysis.columns = ['File_Count', 'Total_Hours', 'Avg_Complexity', 'Total_LOC']\n",
    "        wave_analysis = wave_analysis.reindex([\n",
    "            'Wave 1 - Quick Wins', \n",
    "            'Wave 2 - Standard Migration', \n",
    "            'Wave 3 - Complex Components'\n",
    "        ]).fillna(0)\n",
    "        \n",
    "        print(\"🌊 RECOMMENDED MIGRATION WAVE STRATEGY:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        wave_descriptions = {\n",
    "            'Wave 1 - Quick Wins': {\n",
    "                'emoji': '🟢',\n",
    "                'description': 'Low risk, straightforward migrations to build momentum',\n",
    "                'timeline': '2-4 weeks',\n",
    "                'team': '1-2 developers'\n",
    "            },\n",
    "            'Wave 2 - Standard Migration': {\n",
    "                'emoji': '🟡', \n",
    "                'description': 'Standard complexity migrations with moderate risk',\n",
    "                'timeline': '4-8 weeks',\n",
    "                'team': '2-3 developers'\n",
    "            },\n",
    "            'Wave 3 - Complex Components': {\n",
    "                'emoji': '🔴',\n",
    "                'description': 'High complexity/risk components requiring expert attention',\n",
    "                'timeline': '6-12 weeks',\n",
    "                'team': '3-4 senior developers'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for wave, row in wave_analysis.iterrows():\n",
    "            if pd.isna(row['File_Count']) or row['File_Count'] == 0:\n",
    "                continue\n",
    "                \n",
    "            wave_info = wave_descriptions.get(wave, {'emoji': '⚪', 'description': '', 'timeline': '', 'team': ''})\n",
    "            \n",
    "            print(f\"{wave_info['emoji']} {wave}:\")\n",
    "            print(f\"   📄 Components: {int(row['File_Count'])}\")\n",
    "            print(f\"   ⏱️  Total Hours: {int(row['Total_Hours'])}\")\n",
    "            print(f\"   📊 Avg Complexity: {row['Avg_Complexity']}/10\")\n",
    "            print(f\"   📏 Lines of Code: {int(row['Total_LOC']):,}\")\n",
    "            print(f\"   📅 Timeline: {wave_info['timeline']}\")\n",
    "            print(f\"   👥 Team Size: {wave_info['team']}\")\n",
    "            print(f\"   💡 Strategy: {wave_info['description']}\")\n",
    "            print()\n",
    "        \n",
    "        # High-risk component analysis (if available)\n",
    "        if 'risk_level' in df.columns:\n",
    "            high_risk_files = df[df['risk_level'] == 'High']\n",
    "            \n",
    "            if len(high_risk_files) > 0:\n",
    "                print(\"⚠️  HIGH-RISK COMPONENTS - SPECIAL ATTENTION REQUIRED:\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "                for _, file_data in high_risk_files.iterrows():\n",
    "                    print(f\"🔴 {file_data.get('file_name', 'Unknown')}:\")\n",
    "                    print(f\"   🎯 Complexity: {file_data.get('complexity_score', 'N/A')}/10\")\n",
    "                    print(f\"   ⏱️  Effort: {file_data.get('migration_hours', 'N/A')} hours\")\n",
    "                    print(f\"   🔧 Features: {file_data.get('sql_features', 'Advanced SQL patterns')}\")\n",
    "                    \n",
    "                    # Specific recommendations based on complexity\n",
    "                    complexity = file_data.get('complexity_score', 5.0)\n",
    "                    if complexity > 9:\n",
    "                        print(f\"   💡 Recommendation: Assign senior architect, plan proof-of-concept\")\n",
    "                    else:\n",
    "                        print(f\"   💡 Recommendation: Assign senior developer, thorough testing required\")\n",
    "                    print()\n",
    "        \n",
    "        # Cost-benefit analysis with safe calculations\n",
    "        total_hours = df['migration_hours'].sum() if 'migration_hours' in df.columns else 0\n",
    "        avg_hourly_rate = 150\n",
    "        total_cost = total_hours * avg_hourly_rate\n",
    "        \n",
    "        print(\"💰 COST-BENEFIT ANALYSIS:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"📊 Total Migration Effort: {total_hours} hours\")\n",
    "        print(f\"💵 Estimated Cost: ${total_cost:,} (@ ${avg_hourly_rate}/hour)\")\n",
    "        print(f\"📅 Sequential Timeline: {total_hours/40:.1f} weeks (1 developer)\")\n",
    "        print(f\"📅 Parallel Timeline: {total_hours/160:.1f} weeks (4 developers)\")\n",
    "        print(f\"📅 Recommended Timeline: {total_hours/120:.1f} weeks (3 developers + coordination)\")\n",
    "        \n",
    "        # Expected benefits\n",
    "        print(\"\\n📈 EXPECTED BUSINESS BENEFITS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        benefits = [\n",
    "            (\"Query Performance\", \"3-5x improvement\", \"Faster analytics, better user experience\"),\n",
    "            (\"Analytics Capability\", \"Advanced ML/AI\", \"Predictive supply chain optimization\"),\n",
    "            (\"Infrastructure Cost\", \"20-30% reduction\", \"Cloud-native scaling and optimization\"),\n",
    "            (\"Time-to-Insight\", \"10x faster\", \"Natural language queries with Genie\"),\n",
    "            (\"Scalability\", \"Unlimited scale\", \"Handle peak loads without performance issues\"),\n",
    "            (\"Innovation Speed\", \"2x faster\", \"Rapid prototyping of new analytics\")\n",
    "        ]\n",
    "        \n",
    "        for benefit, improvement, description in benefits:\n",
    "            print(f\"💡 {benefit}: {improvement}\")\n",
    "            print(f\"   {description}\")\n",
    "            print()\n",
    "        \n",
    "        # ROI calculation\n",
    "        annual_savings = 200000  # Estimated annual savings\n",
    "        payback_months = (total_cost / (annual_savings / 12)) if annual_savings > 0 else 0\n",
    "        \n",
    "        print(f\"💰 ROI PROJECTION:\")\n",
    "        print(f\"   Annual Savings Estimate: ${annual_savings:,}\")\n",
    "        print(f\"   Payback Period: {payback_months:.1f} months\")\n",
    "        if total_cost > 0:\n",
    "            print(f\"   3-Year ROI: {((annual_savings * 3 - total_cost) / total_cost * 100):.0f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in migration planning: {e}\")\n",
    "        print(\"This might be due to data format issues, but basic planning completed.\")\n",
    "    \nelse:\n",
    "    print(\"⚠️ Migration planning data not available\")\n",
    "    print(\"💡 Make sure your assessment report includes complexity analysis data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Executive Summary & Next Steps\n",
    "\n",
    "Based on the comprehensive assessment analysis, here are the key findings and actionable next steps for GlobalSupply Corp's data modernization journey to Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary and actionable recommendations with error handling\n",
    "if 'Complexity_Analysis' in sheets_data and len(sheets_data['Complexity_Analysis']) > 0:\n",
    "    df = sheets_data['Complexity_Analysis']\n",
    "    \n",
    "    try:\n",
    "        # Calculate key metrics for summary with safe operations\n",
    "        total_files = len(df)\n",
    "        total_hours = df['migration_hours'].sum() if 'migration_hours' in df.columns else 0\n",
    "        total_cost = total_hours * 150\n",
    "        high_risk_count = len(df[df['risk_level'] == 'High']) if 'risk_level' in df.columns else 0\n",
    "        avg_complexity = df['complexity_score'].mean() if 'complexity_score' in df.columns else 5.0\n",
    "        total_loc = df['lines_of_code'].sum() if 'lines_of_code' in df.columns else 0\n",
    "        \n",
    "        print(\"\"\"\n═══════════════════════════════════════════════════════════════════════════════\n📊 GLOBALSUPPLY CORP - EXECUTIVE SUMMARY & STRATEGIC RECOMMENDATIONS\n═══════════════════════════════════════════════════════════════════════════════\n\n🎯 ASSESSMENT FINDINGS:\n\"\"\")\n",
    "\n        # Key findings with specific numbers\n        findings = [\n",
    "            f\"• {total_files} SQL components analyzed with {total_loc:,} lines of code\",\n",
    "            f\"• Average complexity score: {avg_complexity:.1f}/10 (moderate-to-high complexity)\",\n",
    "            f\"• {high_risk_count} high-risk components requiring expert attention\",\n",
    "            f\"• Estimated migration effort: {total_hours} hours (${total_cost:,})\",\n",
    "            f\"• Strong dependency relationships requiring careful sequencing\"\n",
    "        ]\n",
    "        \n",
    "        for finding in findings:\n",
    "            print(finding)\n",
    "\n",
    "        payback_months = (total_cost / (200000 / 12)) if total_cost > 0 else 0\n",
    "        roi_3year = int(((200000 * 3 - total_cost) / total_cost * 100)) if total_cost > 0 else 0\n",
    "        \n",
    "        print(f\"\"\"\n📈 BUSINESS IMPACT & ROI:\n• Query Performance: 3-5x improvement for analytical workloads\n• Natural Language Queries: Enable business users with Databricks Genie\n• ML/AI Capabilities: Advanced supply chain optimization and forecasting\n• Infrastructure Costs: 20-30% reduction through cloud-native optimization\n• Time-to-Insight: 10x faster analytics development and deployment\n• Scalability: Unlimited scale for peak demand scenarios\n\n💰 FINANCIAL PROJECTIONS:\n• Investment Required: ${total_cost:,}\n• Expected Annual Savings: $200,000+\n• Payback Period: {payback_months:.1f} months\n• 3-Year ROI: {roi_3year}%\"\"\")\n\n        print(\"\"\"\n🚀 STRATEGIC RECOMMENDATIONS:\n\n1. ✅ IMMEDIATE ACTIONS (Next 2 weeks):\n   → Secure executive sponsorship and budget approval\n   → Assemble migration team with SQL Server + Databricks expertise\n   → Set up Databricks workspace and development environment\n   → Begin with Module 2: Schema Migration & Transpilation workshop\n\n2. 📋 SHORT-TERM EXECUTION (4-6 weeks):\n   → Execute Wave 1 migrations (low complexity, quick wins)\n   → Establish CI/CD pipelines for automated testing\n   → Begin user training on Databricks platform\n   → Proceed to Module 3: Data Reconciliation workshop\n\n3. 🎯 MEDIUM-TERM DELIVERY (2-3 months):\n   → Complete Wave 2 & 3 migrations with thorough testing\n   → Implement advanced analytics and ML models\n   → Deploy natural language query capabilities\n   → Complete Module 4: Modern Analytics & ML workshop\n\n4. 🌟 LONG-TERM OPTIMIZATION (3-6 months):\n   → Optimize performance and cost efficiency\n   → Expand ML/AI use cases across supply chain\n   → Train business users on self-service analytics\n   → Plan for additional data sources and use cases\n\n🛠️ CRITICAL SUCCESS FACTORS:\n• Strong project management with clear milestones\n• Dedicated team with both SQL Server and Databricks skills\n• Comprehensive testing strategy including data validation\n• User training and change management program\n• Phased rollout with fallback procedures\n\n📞 RECOMMENDED SUPPORT RESOURCES:\n• Databricks Professional Services for complex components\n• Lakebridge community and documentation\n• Partner ecosystem for specialized migration expertise\n• Training programs for team skill development\"\"\")\n\n        # Risk mitigation strategies\n        if high_risk_count > 0:\n            print(f\"\"\"\n⚠️  RISK MITIGATION FOR {high_risk_count} HIGH-RISK COMPONENTS:\n• Assign senior architects to complex components\n• Develop proof-of-concepts for high-risk migrations\n• Plan for manual testing and validation\n• Consider parallel runs during transition period\n• Maintain rollback procedures for critical systems\"\"\")\n\n        print(\"\"\"\n═══════════════════════════════════════════════════════════════════════════════\n🎯 DECISION: PROCEED WITH DATABRICKS MIGRATION\n\nThe assessment demonstrates a strong business case for migrating GlobalSupply Corp's\ndata warehouse to Databricks. The combination of performance improvements, cost \nsavings, and advanced analytics capabilities provides compelling ROI.\n\nNext Workshop Module: Schema Migration & Transpilation\n═══════════════════════════════════════════════════════════════════════════════\n\"\"\")\n    \n    except Exception as e:\n        print(f\"❌ Error generating executive summary: {e}\")\n        print(\"This might be due to missing data columns, but assessment analysis was successful.\")\n    \nelse:\n    print(\"⚠️ Assessment data not available for executive summary\")\n    print(\"💡 Make sure you have run the assessment analyzer first:\")\n    print(\"   python 01_assessment_analyzer.py --generate-samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📤 Export Results for Stakeholders\n",
    "\n",
    "Generate reports for different stakeholder groups:\n",
    "- **Executive Summary** - High-level findings and recommendations\n",
    "- **Technical Report** - Detailed migration plan with component breakdown\n",
    "- **Project Plan** - Timeline, resources, and milestone tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comprehensive results for different stakeholder groups with error handling\n",
    "if 'Complexity_Analysis' in sheets_data and len(sheets_data['Complexity_Analysis']) > 0:\n",
    "    df = sheets_data['Complexity_Analysis'].copy()\n",
    "    \n",
    "    try:\n",
    "        # Ensure migration wave assignments exist\n",
    "        if 'Migration_Wave' not in df.columns:\n",
    "            def assign_migration_wave(row):\n",
    "                risk = row.get('risk_level', 'Medium')\n",
    "                complexity = row.get('complexity_score', 5.0)\n",
    "                \n",
    "                if risk == 'Low' and complexity < 6:\n",
    "                    return 'Wave 1 - Quick Wins'\n",
    "                elif risk == 'Medium' or (risk == 'Low' and complexity >= 6):\n",
    "                    return 'Wave 2 - Standard Migration'\n",
    "                else:\n",
    "                    return 'Wave 3 - Complex Components'\n",
    "                    \n",
    "            df['Migration_Wave'] = df.apply(assign_migration_wave, axis=1)\n",
    "        \n",
    "        # 1. Export detailed technical migration plan\n",
    "        try:\n",
    "            df.to_csv('globalsupply_detailed_migration_plan.csv', index=False)\n",
    "            print(\"✅ Detailed technical plan exported: globalsupply_detailed_migration_plan.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not export CSV: {e}\")\n",
    "        \n",
    "        # Calculate summary metrics safely\n",
    "        total_files = len(df)\n",
    "        total_hours = df['migration_hours'].sum() if 'migration_hours' in df.columns else 0\n",
    "        total_cost = total_hours * 150\n",
    "        high_risk_count = len(df[df['risk_level'] == 'High']) if 'risk_level' in df.columns else 0\n",
    "        total_loc = df['lines_of_code'].sum() if 'lines_of_code' in df.columns else 0\n",
    "        table_refs = df['table_references'].sum() if 'table_references' in df.columns else 0\n",
    "        \n",
    "        # 2. Create executive summary document\n",
    "        exec_summary = f\"\"\"\nGLOBALSUPPLY CORP - DATABRICKS MIGRATION ASSESSMENT\nExecutive Summary Report\nGenerated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n========================================================\n\nPROJECT SCOPE:\n• {total_files} SQL components analyzed\n• {total_loc:,} total lines of code\n• {table_refs} database table dependencies\n• Supply chain analytics and reporting workloads\n\nINVESTMENT REQUIRED:\n• Development Effort: {total_hours} hours\n• Estimated Cost: ${total_cost:,} (including team, tools, training)\n• Timeline: {total_hours/120:.1f} weeks with 3-person team\n• Phased Approach: 3 migration waves over 4-5 months\n\nRISK ASSESSMENT:\n• {high_risk_count} high-risk components requiring expert attention\n• {len(df[df['risk_level'] == 'Medium']) if 'risk_level' in df.columns else 0} medium-risk components for standard migration\n• {len(df[df['risk_level'] == 'Low']) if 'risk_level' in df.columns else 0} low-risk components for quick wins\n• Comprehensive dependency mapping completed\n• Mitigation strategies defined for all risk categories\n\nBUSINESS BENEFITS:\n• Performance: 3-5x improvement in query execution\n• Analytics: Advanced ML/AI capabilities for supply chain optimization\n• User Experience: Natural language queries with Databricks Genie\n• Cost Efficiency: 20-30% reduction in infrastructure costs\n• Scalability: Unlimited scale for peak demand periods\n• Innovation: 2x faster development of new analytics\n\nFINANCIAL PROJECTIONS:\n• Annual Cost Savings: $200,000+ (infrastructure + productivity)\n• Payback Period: {(total_cost / (200000 / 12)):.1f} months\n• 3-Year Net Present Value: ${(200000 * 3 - total_cost):,}\n• ROI: {((200000 * 3 - total_cost) / total_cost * 100):.0f}% over 3 years\n\nSTRATEGIC RECOMMENDATION:\nPROCEED with Databricks migration using Lakebridge toolchain.\nThe analysis demonstrates strong business justification with\nmanageable technical risk and clear path to success.\n\nKEY SUCCESS FACTORS:\n• Executive sponsorship and dedicated team\n• Phased migration approach starting with quick wins\n• Comprehensive testing and validation procedures\n• User training and change management program\n• Partnership with Databricks Professional Services\n\nNEXT STEPS:\n1. Secure budget approval and team assignment\n2. Set up Databricks workspace and development environment  \n3. Begin Schema Migration & Transpilation workshop (Module 2)\n4. Execute Wave 1 migrations within 4-6 weeks\n\nThis assessment provides the foundation for a successful\nmodernization that will transform GlobalSupply Corp's \nanalytics capabilities and competitive advantage.\n\"\"\"\n        \n        try:\n            with open('globalsupply_executive_summary.txt', 'w') as f:\n                f.write(exec_summary)\n            print(\"✅ Executive summary exported: globalsupply_executive_summary.txt\")\n        except Exception as e:\n            print(f\"⚠️ Could not export executive summary: {e}\")\n        \n        # 3. Create project tracking template\n        if 'Migration_Wave' in df.columns:\n            try:\n                wave_summary = df.groupby('Migration_Wave').agg({\n",
    "                    'file_name': 'count',\n",
    "                    'migration_hours': 'sum' if 'migration_hours' in df.columns else lambda x: 0,\n",
    "                    'complexity_score': 'mean' if 'complexity_score' in df.columns else lambda x: 5.0\n",
    "                }).round(1)\n",
    "                \n",
    "                wave_summary.to_csv('globalsupply_project_waves.csv')\n",
    "                print(\"✅ Project wave summary exported: globalsupply_project_waves.csv\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not export wave summary: {e}\")\n",
    "        \n",
    "        print(\"\\n📊 ASSESSMENT COMPLETE - Ready for Next Phase\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"🎯 Deliverables Generated:\")\n",
    "        print(\"   📄 Executive Summary (for leadership team)\")\n",
    "        print(\"   📋 Detailed Migration Plan (for technical team)\")\n",
    "        print(\"   📊 Project Wave Summary (for project managers)\")\n",
    "        print(\"\\n🚀 Next Workshop Module: Schema Migration & Transpilation\")\n",
    "        print(\"   Location: ../02_transpilation/\")\n",
    "        print(\"   Focus: Hands-on SQL conversion using Lakebridge\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during export process: {e}\")\n",
    "        print(\"Assessment analysis was successful, but export encountered issues.\")\n",
    "        \nelse:\n",
    "    print(\"⚠️ No assessment data available for export\")\n",
    "    print(\"\\n📝 To generate real assessment data:\")\n",
    "    print(\"   1. Export your SQL Server workloads to files\")\n",
    "    print(\"   2. Run: python 01_assessment_analyzer.py --generate-samples\")\n",
    "    print(\"   3. Re-run this notebook with the generated Excel report\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}