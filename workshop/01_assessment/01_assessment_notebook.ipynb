{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GlobalSupply Corp - Migration Assessment Analysis\n",
    "\n",
    "## üè¢ Business Context\n",
    "**GlobalSupply Corp** operates a complex supply chain data warehouse built on SQL Server with TPC-H style schemas. This workshop demonstrates how to modernize this legacy system to Databricks using Lakebridge for:\n",
    "\n",
    "- **AI-powered supply chain optimization**\n",
    "- **Natural language queries for business users**  \n",
    "- **Real-time analytics and forecasting**\n",
    "- **Scalable cloud-native architecture**\n",
    "\n",
    "## üìä What This Notebook Covers\n",
    "- **Migration Complexity Analysis** - Which components require the most effort\n",
    "- **Dependency Mapping** - Critical interdependencies affecting migration sequencing\n",
    "- **Risk Assessment** - Potential challenges and mitigation strategies\n",
    "- **Business Value Analysis** - ROI projections for Databricks migration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üîß Prerequisites & Setup\n\n### Step 1: Install Required Python Dependencies\n```bash\n# Install essential packages for analysis and Excel support\npip install pandas matplotlib seaborn numpy openpyxl\n```\n\n**Note:** `openpyxl` is required to read Excel files generated by Lakebridge Analyzer.\n\n### Step 2: Install Lakebridge (Optional - for real assessments)\n```bash\n# Install Databricks Labs Lakebridge\ndatabricks labs install lakebridge\n\n# Verify installation\ndatabricks labs lakebridge analyze --help\n```\n\n### Step 3: Prepare Legacy SQL Files\nFor real assessments, export your SQL Server workloads to a directory structure like:\n```\nlegacy_sql/\n‚îú‚îÄ‚îÄ analytics/\n‚îÇ   ‚îú‚îÄ‚îÄ supply_chain_performance.sql\n‚îÇ   ‚îú‚îÄ‚îÄ inventory_optimization.sql\n‚îÇ   ‚îî‚îÄ‚îÄ customer_profitability.sql\n‚îú‚îÄ‚îÄ reports/\n‚îÇ   ‚îú‚îÄ‚îÄ financial_summary.sql\n‚îÇ   ‚îî‚îÄ‚îÄ operational_reports.sql\n‚îî‚îÄ‚îÄ etl/\n    ‚îú‚îÄ‚îÄ data_processing.sql\n    ‚îî‚îÄ‚îÄ transformations.sql\n```\n\n### Step 4: Run Assessment (with sample data or real SQL files)\n```bash\n# Option 1: Use sample SQL files for demonstration\npython 01_assessment_analyzer.py --generate-samples\n\n# Option 2: Assess real SQL files\npython 01_assessment_analyzer.py --source-directory /path/to/legacy_sql\n\n# Option 3: Use Lakebridge directly (if installed and configured)\ndatabricks labs lakebridge analyze \\\n  --source-directory /path/to/legacy_sql \\\n  --report-file globalsupply_assessment \\\n  --source-tech mssql\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Understanding Lakebridge Analyzer Output\n",
    "\n",
    "The Lakebridge Analyzer generates comprehensive insights including:\n",
    "\n",
    "### üîç Analysis Insights\n",
    "- **Job Complexity Assessment** - Complexity scores (1-10) for each SQL component\n",
    "- **Comprehensive Job Inventory** - All mappings, transformations, functions cataloged\n",
    "- **Cross-System Interdependency Mapping** - Shows how components interact\n",
    "- **Migration Effort Estimates** - Engineering hours required per component\n",
    "\n",
    "### üìä Key Outputs\n",
    "- **Excel Report** with multiple worksheets:\n",
    "  - Complexity Analysis\n",
    "  - Dependency Mapping  \n",
    "  - Function Usage Statistics\n",
    "  - Migration Estimates\n",
    "  - Risk Assessment\n",
    "\n",
    "### üéØ Business Value\n",
    "- **Risk Assessment** for migration planning\n",
    "- **Resource Planning** for modernization project  \n",
    "- **Sequencing Guidance** to minimize disruption\n",
    "- **TCO Analysis** for Databricks migration ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for assessment analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting style for professional reports\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(\"üìä Ready for GlobalSupply Corp assessment analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Load Assessment Results\n",
    "\n",
    "The Lakebridge Analyzer generates a comprehensive Excel report. This section loads and processes that data for analysis.\n",
    "\n",
    "### Expected Excel Worksheets:\n",
    "- **Summary** - High-level overview\n",
    "- **Complexity Analysis** - Detailed complexity scores\n",
    "- **Dependencies** - Component interdependencies\n",
    "- **Functions** - SQL function usage statistics\n",
    "- **Migration Estimates** - Effort and timeline projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_assessment_report():\n",
    "    \"\"\"\n",
    "    Find the most recent Lakebridge assessment report\n",
    "    \"\"\"\n",
    "    current_dir = Path('.')\n",
    "    \n",
    "    # Look for assessment reports with common naming patterns\n",
    "    patterns = ['*assessment*.xlsx', '*globalsupply*.xlsx', '*remorph*.xlsx']\n",
    "    excel_files = []\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        excel_files.extend(list(current_dir.glob(pattern)))\n",
    "    \n",
    "    if not excel_files:\n",
    "        # Look for any Excel files\n",
    "        excel_files = list(current_dir.glob('*.xlsx'))\n",
    "    \n",
    "    if excel_files:\n",
    "        # Return the most recent file\n",
    "        latest_file = max(excel_files, key=lambda x: x.stat().st_mtime)\n",
    "        return latest_file\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Attempt to load the assessment report\n",
    "report_file = find_assessment_report()\n",
    "\n",
    "if report_file:\n",
    "    print(f\"üìÑ Found assessment report: {report_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the Excel file and examine structure\n",
    "        excel_data = pd.ExcelFile(report_file)\n",
    "        print(f\"üìã Available worksheets: {excel_data.sheet_names}\")\n",
    "        \n",
    "        # Load key worksheets\n",
    "        sheets_data = {}\n",
    "        for sheet_name in excel_data.sheet_names:\n",
    "            try:\n",
    "                sheets_data[sheet_name] = pd.read_excel(report_file, sheet_name=sheet_name)\n",
    "                print(f\"‚úÖ Loaded '{sheet_name}': {len(sheets_data[sheet_name])} rows, {len(sheets_data[sheet_name].columns)} columns\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not load sheet '{sheet_name}': {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Excel file: {e}\")\n",
    "        sheets_data = {}\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No assessment report found!\")\n",
    "    print(\"\")\n",
    "    print(\"üìù To generate an assessment report:\")\n",
    "    print(\"   1. Ensure Lakebridge is installed: databricks labs install lakebridge\")\n",
    "    print(\"   2. Run assessment: databricks labs lakebridge analyze --source-directory /path/to/sql --source-tech mssql\")\n",
    "    print(\"   3. Place the generated .xlsx file in this directory\")\n",
    "    print(\"\")\n",
    "    print(\"üé≠ For demonstration purposes, we'll create sample data...\")\n",
    "    sheets_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Sample Data for Demonstration\n",
    "\n",
    "If no real assessment report is available, we'll create representative sample data that demonstrates typical findings from a GlobalSupply Corp assessment.\n",
    "\n",
    "### Sample Scenario:\n",
    "- **8 SQL files** representing typical supply chain analytics\n",
    "- **Mixed complexity levels** from simple reports to complex analytics\n",
    "- **Realistic dependencies** between components\n",
    "- **Representative migration effort estimates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic sample data if no real assessment report exists\n",
    "if not sheets_data:\n",
    "    print(\"üé≠ Creating sample GlobalSupply Corp assessment data for demonstration...\")\n",
    "    \n",
    "    # Sample complexity analysis representing real-world SQL Server workloads\n",
    "    complexity_data = pd.DataFrame({\n",
    "        'File_Name': [\n",
    "            'supply_chain_performance.sql',\n",
    "            'inventory_optimization.sql', \n",
    "            'customer_profitability.sql',\n",
    "            'supplier_risk_assessment.sql',\n",
    "            'dynamic_reporting.sql',\n",
    "            'window_functions_analysis.sql',\n",
    "            'financial_summary.sql',\n",
    "            'order_processing.sql'\n",
    "        ],\n",
    "        'Lines_of_Code': [145, 198, 167, 223, 89, 134, 67, 89],\n",
    "        'Complexity_Score': [8.5, 9.2, 7.8, 9.8, 6.5, 7.2, 4.5, 5.1],\n",
    "        'Functions_Used': [12, 18, 14, 22, 8, 16, 6, 9],\n",
    "        'Table_References': [8, 6, 7, 9, 5, 4, 3, 4],\n",
    "        'Migration_Hours': [16, 24, 18, 32, 8, 12, 4, 6],\n",
    "        'Risk_Level': ['Medium', 'High', 'Medium', 'High', 'Low', 'Medium', 'Low', 'Low'],\n",
    "        'Category': ['Analytics', 'Analytics', 'Analytics', 'Analytics', 'Reporting', 'Analytics', 'Reporting', 'OLTP'],\n",
    "        'SQL_Features': [\n",
    "            'CTEs, Window Functions, Complex Joins',\n",
    "            'Recursive CTEs, PIVOT, Advanced Analytics', \n",
    "            'PIVOT, Window Functions, String Aggregation',\n",
    "            'Recursive CTEs, Dynamic SQL, Risk Scoring',\n",
    "            'Dynamic SQL, Conditional Logic',\n",
    "            'Advanced Window Functions, LAG/LEAD',\n",
    "            'Basic Aggregation, Simple Joins',\n",
    "            'CRUD Operations, Transactions'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Sample dependency mapping\n",
    "    dependency_data = pd.DataFrame({\n",
    "        'Source_Object': [\n",
    "            'customer_profitability.sql', \n",
    "            'supplier_risk_assessment.sql', \n",
    "            'inventory_optimization.sql',\n",
    "            'supply_chain_performance.sql',\n",
    "            'window_functions_analysis.sql'\n",
    "        ],\n",
    "        'Target_Object': [\n",
    "            'financial_summary.sql', \n",
    "            'supply_chain_performance.sql', \n",
    "            'order_processing.sql',\n",
    "            'dynamic_reporting.sql',\n",
    "            'customer_profitability.sql'\n",
    "        ],\n",
    "        'Dependency_Type': ['View', 'Procedure', 'Table', 'View', 'Function'],\n",
    "        'Criticality': ['High', 'High', 'Medium', 'Medium', 'Low']\n",
    "    })\n",
    "    \n",
    "    # Sample function usage statistics\n",
    "    function_data = pd.DataFrame({\n",
    "        'Function_Name': [\n",
    "            'ROW_NUMBER', 'RANK', 'LAG', 'LEAD', 'SUM', 'AVG', 'COUNT', 'DATEDIFF', \n",
    "            'DATEADD', 'STRING_AGG', 'PIVOT', 'CASE', 'CTE', 'RECURSIVE_CTE'\n",
    "        ],\n",
    "        'Usage_Count': [15, 12, 8, 6, 25, 20, 30, 18, 14, 5, 3, 22, 18, 2],\n",
    "        'Complexity_Impact': [3, 3, 4, 4, 1, 1, 1, 2, 2, 4, 5, 2, 3, 5],\n",
    "        'Databricks_Compatibility': [\n",
    "            'Direct', 'Direct', 'Direct', 'Direct', 'Direct', 'Direct', 'Direct', 'Modified',\n",
    "            'Modified', 'Modified', 'Modified', 'Direct', 'Direct', 'Complex'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    sheets_data = {\n",
    "        'Complexity_Analysis': complexity_data,\n",
    "        'Dependencies': dependency_data,\n",
    "        'Function_Usage': function_data\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ Sample data created with realistic GlobalSupply Corp scenarios\")\n",
    "    print(f\"üìä Generated {len(complexity_data)} SQL components for analysis\")\n",
    "    print(f\"üîó Created {len(dependency_data)} dependency relationships\")\n",
    "    print(f\"üîß Analyzed {len(function_data)} SQL functions\")\n",
    "\n",
    "# Display basic statistics about the loaded data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä ASSESSMENT DATA SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for sheet_name, df in sheets_data.items():\n",
    "    print(f\"üìã {sheet_name}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    if len(df.columns) > 0:\n",
    "        print(f\"   Columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Assessment Overview Dashboard\n",
    "\n",
    "This section provides a high-level overview of the migration assessment, showing key metrics that executives and project managers need to understand the scope and complexity of the modernization effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive assessment overview\n",
    "if 'Complexity_Analysis' in sheets_data:\n",
    "    df = sheets_data['Complexity_Analysis']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä GLOBALSUPPLY CORP - MIGRATION ASSESSMENT OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Key metrics\n",
    "    total_files = len(df)\n",
    "    total_loc = df['Lines_of_Code'].sum()\n",
    "    total_hours = df['Migration_Hours'].sum()\n",
    "    avg_complexity = df['Complexity_Score'].mean()\n",
    "    \n",
    "    print(f\"üìÅ Total SQL Components: {total_files}\")\n",
    "    print(f\"üìè Total Lines of Code: {total_loc:,}\")\n",
    "    print(f\"‚è±Ô∏è  Estimated Migration Hours: {total_hours}\")\n",
    "    print(f\"üìà Average Complexity Score: {avg_complexity:.1f}/10\")\n",
    "    print(f\"üí∞ Estimated Cost (@$150/hour): ${total_hours * 150:,}\")\n",
    "    \n",
    "    # Risk distribution analysis\n",
    "    risk_distribution = df['Risk_Level'].value_counts()\n",
    "    print(f\"\\nüö¶ RISK DISTRIBUTION:\")\n",
    "    for risk, count in risk_distribution.items():\n",
    "        percentage = (count/len(df)*100)\n",
    "        risk_emoji = {'Low': 'üü¢', 'Medium': 'üü°', 'High': 'üî¥'}\n",
    "        print(f\"   {risk_emoji.get(risk, '‚ö™')} {risk}: {count} files ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Category distribution\n",
    "    category_distribution = df['Category'].value_counts()\n",
    "    print(f\"\\nüìä WORKLOAD CATEGORIES:\")\n",
    "    for category, count in category_distribution.items():\n",
    "        percentage = (count/len(df)*100)\n",
    "        print(f\"   üìà {category}: {count} files ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Timeline estimates\n",
    "    weeks_single = total_hours / 40\n",
    "    weeks_team = total_hours / 160  # 4-person team\n",
    "    \n",
    "    print(f\"\\nüìÖ TIMELINE ESTIMATES:\")\n",
    "    print(f\"   üë§ Single Developer: {weeks_single:.1f} weeks\")\n",
    "    print(f\"   üë• 4-Person Team: {weeks_team:.1f} weeks\")\n",
    "    \n",
    "    print(\"\\nüí° KEY INSIGHTS:\")\n",
    "    high_risk_count = len(df[df['Risk_Level'] == 'High'])\n",
    "    complex_count = len(df[df['Complexity_Score'] > 8])\n",
    "    \n",
    "    if high_risk_count > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  {high_risk_count} high-risk components need expert attention\")\n",
    "    if complex_count > 0:\n",
    "        print(f\"   üß† {complex_count} highly complex components (>8/10 complexity)\")\n",
    "    print(f\"   üéØ Recommended phased approach in 3 migration waves\")\n",
    "    print(f\"   üìà Expected 3-5x performance improvement with Databricks\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Complexity analysis data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Complexity Analysis Visualizations\n",
    "\n",
    "These visualizations help identify which components will require the most attention during migration and provide insights into the overall complexity distribution of the SQL codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Complexity_Analysis' in sheets_data:\n",
    "    df = sheets_data['Complexity_Analysis']\n",
    "    \n",
    "    # Create comprehensive complexity dashboard\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('GlobalSupply Corp - Migration Complexity Analysis Dashboard', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # 1. Complexity Score Distribution\n",
    "    axes[0, 0].hist(df['Complexity_Score'], bins=10, alpha=0.7, color='skyblue', \n",
    "                    edgecolor='black', linewidth=1.2)\n",
    "    axes[0, 0].set_title('üìä Complexity Score Distribution', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Complexity Score (1-10 scale)')\n",
    "    axes[0, 0].set_ylabel('Number of SQL Components')\n",
    "    \n",
    "    # Add mean line and statistics\n",
    "    mean_complexity = df['Complexity_Score'].mean()\n",
    "    axes[0, 0].axvline(mean_complexity, color='red', linestyle='--', linewidth=2,\n",
    "                       label=f'Mean: {mean_complexity:.1f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Add complexity thresholds\n",
    "    axes[0, 0].axvline(6, color='orange', linestyle=':', alpha=0.7, label='Medium Risk')\n",
    "    axes[0, 0].axvline(8, color='red', linestyle=':', alpha=0.7, label='High Risk')\n",
    "    \n",
    "    # 2. Risk Level Distribution (Pie Chart)\n",
    "    risk_counts = df['Risk_Level'].value_counts()\n",
    "    colors = {'Low': '#2ecc71', 'Medium': '#f39c12', 'High': '#e74c3c'}\n",
    "    risk_colors = [colors[risk] for risk in risk_counts.index]\n",
    "    \n",
    "    wedges, texts, autotexts = axes[0, 1].pie(risk_counts.values, labels=risk_counts.index, \n",
    "                                              autopct='%1.1f%%', colors=risk_colors, \n",
    "                                              startangle=90, explode=(0.05, 0.05, 0.05))\n",
    "    axes[0, 1].set_title('üö¶ Migration Risk Distribution', fontweight='bold')\n",
    "    \n",
    "    # Enhance pie chart text\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    # 3. Effort vs Complexity Scatter Plot\n",
    "    scatter = axes[1, 0].scatter(df['Complexity_Score'], df['Migration_Hours'], \n",
    "                                c=df['Lines_of_Code'], cmap='viridis', alpha=0.8, \n",
    "                                s=150, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Complexity Score')\n",
    "    axes[1, 0].set_ylabel('Migration Hours')\n",
    "    axes[1, 0].set_title('‚ö° Effort vs Complexity (bubble size = Lines of Code)', fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    cbar.set_label('Lines of Code', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df['Complexity_Score'], df['Migration_Hours'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1, 0].plot(df['Complexity_Score'], p(df['Complexity_Score']), \"r--\", alpha=0.7)\n",
    "    \n",
    "    # 4. Category-wise Migration Hours (Horizontal Bar Chart)\n",
    "    category_hours = df.groupby('Category')['Migration_Hours'].sum().sort_values(ascending=True)\n",
    "    bars = axes[1, 1].barh(category_hours.index, category_hours.values, \n",
    "                           color=['#3498db', '#e67e22', '#9b59b6'], alpha=0.8)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Total Migration Hours')\n",
    "    axes[1, 1].set_title('üìä Migration Effort by Category', fontweight='bold')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value) in enumerate(zip(bars, category_hours.values)):\n",
    "        axes[1, 1].text(value + 0.5, i, f'{int(value)}h', \n",
    "                         va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional insights table\n",
    "    print(\"\\nüìã DETAILED COMPLEXITY BREAKDOWN:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Show top 5 most complex components\n",
    "    top_complex = df.nlargest(5, 'Complexity_Score')[['File_Name', 'Complexity_Score', \n",
    "                                                       'Migration_Hours', 'SQL_Features']]\n",
    "    \n",
    "    for _, row in top_complex.iterrows():\n",
    "        print(f\"üìÑ {row['File_Name']}:\")\n",
    "        print(f\"   üéØ Complexity: {row['Complexity_Score']}/10\")\n",
    "        print(f\"   ‚è±Ô∏è  Effort: {row['Migration_Hours']} hours\")\n",
    "        print(f\"   üîß Features: {row['SQL_Features']}\")\n",
    "        print()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Complexity analysis data not available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Dependency Analysis\n",
    "\n",
    "Understanding dependencies is crucial for migration sequencing. This analysis shows which components depend on others and helps plan the migration order to minimize risk and ensure system stability throughout the process.\n",
    "\n",
    "### Why Dependencies Matter:\n",
    "- **Migration Sequencing** - Dependencies must be migrated before dependent components\n",
    "- **Risk Management** - High-criticality dependencies require extra attention\n",
    "- **Testing Strategy** - Dependent components need integrated testing\n",
    "- **Rollback Planning** - Understanding dependencies helps plan safe rollback procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Dependencies' in sheets_data:\n",
    "    dep_df = sheets_data['Dependencies']\n",
    "    \n",
    "    print(\"üîó DEPENDENCY ANALYSIS FOR MIGRATION PLANNING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if len(dep_df) > 0:\n",
    "        print(f\"üìä Total Dependencies Identified: {len(dep_df)}\")\n",
    "        \n",
    "        # Analyze dependency criticality\n",
    "        if 'Criticality' in dep_df.columns:\n",
    "            criticality_counts = dep_df['Criticality'].value_counts()\n",
    "            print(\"\\nüö¶ Dependency Criticality Breakdown:\")\n",
    "            for crit, count in criticality_counts.items():\n",
    "                crit_emoji = {'High': 'üî¥', 'Medium': 'üü°', 'Low': 'üü¢'}\n",
    "                percentage = (count / len(dep_df) * 100)\n",
    "                print(f\"   {crit_emoji.get(crit, '‚ö™')} {crit}: {count} dependencies ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Analyze dependency types\n",
    "        if 'Dependency_Type' in dep_df.columns:\n",
    "            type_counts = dep_df['Dependency_Type'].value_counts()\n",
    "            print(\"\\nüìã Dependency Types:\")\n",
    "            for dep_type, count in type_counts.items():\n",
    "                print(f\"   üìä {dep_type}: {count} dependencies\")\n",
    "        \n",
    "        print(\"\\nüîó Critical Dependency Chains:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Show high-criticality dependencies first\n",
    "        high_crit_deps = dep_df[dep_df['Criticality'] == 'High'] if 'Criticality' in dep_df.columns else dep_df.head(5)\n",
    "        \n",
    "        for _, row in high_crit_deps.iterrows():\n",
    "            source = row['Source_Object'].replace('.sql', '')\n",
    "            target = row['Target_Object'].replace('.sql', '')\n",
    "            dep_type = row.get('Dependency_Type', 'Unknown')\n",
    "            criticality = row.get('Criticality', 'Unknown')\n",
    "            \n",
    "            print(f\"   {source} ‚ûú {target}\")\n",
    "            print(f\"     Type: {dep_type} | Criticality: {criticality}\")\n",
    "            print()\n",
    "        \n",
    "        # Migration sequencing recommendations\n",
    "        print(\"üìã MIGRATION SEQUENCING RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Find components with no dependencies (good starting points)\n",
    "        all_sources = set(dep_df['Source_Object'])\n",
    "        all_targets = set(dep_df['Target_Object'])\n",
    "        independent_components = all_sources - all_targets\n",
    "        \n",
    "        if independent_components:\n",
    "            print(\"üèÅ Start with these independent components:\")\n",
    "            for comp in list(independent_components)[:3]:\n",
    "                print(f\"   ‚úÖ {comp.replace('.sql', '')}\")\n",
    "        \n",
    "        # Find components that many others depend on\n",
    "        dependency_counts = dep_df['Source_Object'].value_counts()\n",
    "        if len(dependency_counts) > 0:\n",
    "            print(\"\\nüéØ High-priority components (many dependencies):\")\n",
    "            for comp, count in dependency_counts.head(3).items():\n",
    "                print(f\"   üî¥ {comp.replace('.sql', '')} ({count} dependent components)\")\n",
    "    \n",
    "    # Create dependency visualizations\n",
    "    if len(dep_df) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle('GlobalSupply Corp - Dependency Analysis', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 1. Criticality distribution\n",
    "        if 'Criticality' in dep_df.columns:\n",
    "            criticality_counts = dep_df['Criticality'].value_counts()\n",
    "            colors = {'High': '#e74c3c', 'Medium': '#f39c12', 'Low': '#2ecc71'}\n",
    "            crit_colors = [colors.get(crit, '#95a5a6') for crit in criticality_counts.index]\n",
    "            \n",
    "            wedges, texts, autotexts = ax1.pie(criticality_counts.values, \n",
    "                                               labels=criticality_counts.index,\n",
    "                                               autopct='%1.1f%%', colors=crit_colors,\n",
    "                                               startangle=90, explode=(0.05, 0.05, 0.05))\n",
    "            ax1.set_title('üö¶ Dependency Criticality', fontweight='bold')\n",
    "            \n",
    "            for autotext in autotexts:\n",
    "                autotext.set_color('white')\n",
    "                autotext.set_fontweight('bold')\n",
    "        \n",
    "        # 2. Dependency type distribution\n",
    "        if 'Dependency_Type' in dep_df.columns:\n",
    "            type_counts = dep_df['Dependency_Type'].value_counts()\n",
    "            bars = ax2.bar(type_counts.index, type_counts.values, \n",
    "                          color=['#3498db', '#e67e22', '#9b59b6', '#1abc9c'][:len(type_counts)])\n",
    "            \n",
    "            ax2.set_title('üìä Dependencies by Type', fontweight='bold')\n",
    "            ax2.set_xlabel('Dependency Type')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, type_counts.values):\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                         str(value), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dependency analysis data not available\")\n",
    "    print(\"üí° In a real assessment, Lakebridge would identify:\")\n",
    "    print(\"   ‚Ä¢ Table-to-table dependencies\")\n",
    "    print(\"   ‚Ä¢ View-to-table relationships\")\n",
    "    print(\"   ‚Ä¢ Stored procedure call chains\")\n",
    "    print(\"   ‚Ä¢ Cross-database dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß SQL Function Analysis\n",
    "\n",
    "This analysis examines SQL functions and features used in the legacy code to understand:\n",
    "- **Databricks Compatibility** - Which functions translate directly vs need modification\n",
    "- **Complexity Drivers** - Functions that contribute most to migration complexity\n",
    "- **Focus Areas** - Where development effort should be concentrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Function_Usage' in sheets_data:\n",
    "    func_df = sheets_data['Function_Usage']\n",
    "    \n",
    "    print(\"üîß SQL FUNCTION COMPATIBILITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_functions = func_df['Usage_Count'].sum()\n",
    "    unique_functions = len(func_df)\n",
    "    \n",
    "    print(f\"üìä Total Function Usages: {total_functions}\")\n",
    "    print(f\"üîß Unique Functions: {unique_functions}\")\n",
    "    \n",
    "    # Compatibility breakdown\n",
    "    if 'Databricks_Compatibility' in func_df.columns:\n",
    "        compat_counts = func_df['Databricks_Compatibility'].value_counts()\n",
    "        print(\"\\nüéØ DATABRICKS COMPATIBILITY BREAKDOWN:\")\n",
    "        \n",
    "        compat_emojis = {\n",
    "            'Direct': '‚úÖ Direct translation',\n",
    "            'Modified': 'üîÑ Requires modification', \n",
    "            'Complex': '‚ö†Ô∏è Complex migration',\n",
    "            'Manual': 'üõ†Ô∏è Manual rewrite needed'\n",
    "        }\n",
    "        \n",
    "        for compat, count in compat_counts.items():\n",
    "            percentage = (count / len(func_df) * 100)\n",
    "            emoji_desc = compat_emojis.get(compat, f'üîç {compat}')\n",
    "            print(f\"   {emoji_desc}: {count} functions ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Top complexity drivers\n",
    "    if 'Complexity_Impact' in func_df.columns:\n",
    "        print(\"\\nüéØ TOP COMPLEXITY DRIVERS:\")\n",
    "        complexity_drivers = func_df.nlargest(5, 'Complexity_Impact')\n",
    "        \n",
    "        for _, row in complexity_drivers.iterrows():\n",
    "            function = row['Function_Name']\n",
    "            usage = row['Usage_Count']\n",
    "            complexity = row['Complexity_Impact']\n",
    "            compat = row.get('Databricks_Compatibility', 'Unknown')\n",
    "            \n",
    "            print(f\"   üîß {function}:\")\n",
    "            print(f\"      Used {usage} times | Complexity: {complexity}/5 | Compatibility: {compat}\")\n",
    "    \n",
    "    # Most used functions\n",
    "    print(\"\\nüìà MOST FREQUENTLY USED FUNCTIONS:\")\n",
    "    top_used = func_df.nlargest(5, 'Usage_Count')\n",
    "    \n",
    "    for _, row in top_used.iterrows():\n",
    "        function = row['Function_Name']\n",
    "        usage = row['Usage_Count']\n",
    "        compat = row.get('Databricks_Compatibility', 'Unknown')\n",
    "        \n",
    "        status_emoji = {'Direct': '‚úÖ', 'Modified': 'üîÑ', 'Complex': '‚ö†Ô∏è'}.get(compat, 'üîç')\n",
    "        print(f\"   {status_emoji} {function}: {usage} usages ({compat})\")\n",
    "    \n",
    "    # Create function analysis visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('SQL Function Usage & Compatibility Analysis', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Compatibility distribution\n",
    "    if 'Databricks_Compatibility' in func_df.columns:\n",
    "        compat_counts = func_df['Databricks_Compatibility'].value_counts()\n",
    "        colors = {'Direct': '#2ecc71', 'Modified': '#f39c12', 'Complex': '#e74c3c', 'Manual': '#8e44ad'}\n",
    "        compat_colors = [colors.get(comp, '#95a5a6') for comp in compat_counts.index]\n",
    "        \n",
    "        ax1.pie(compat_counts.values, labels=compat_counts.index, autopct='%1.1f%%',\n",
    "                colors=compat_colors, startangle=90)\n",
    "        ax1.set_title('üéØ Compatibility Distribution')\n",
    "    \n",
    "    # 2. Top used functions\n",
    "    top_used = func_df.nlargest(8, 'Usage_Count')\n",
    "    bars = ax2.barh(top_used['Function_Name'], top_used['Usage_Count'], color='skyblue')\n",
    "    ax2.set_title('üìä Most Used Functions')\n",
    "    ax2.set_xlabel('Usage Count')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, top_used['Usage_Count']):\n",
    "        ax2.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2,\n",
    "                 str(value), va='center', fontweight='bold')\n",
    "    \n",
    "    # 3. Complexity vs Usage scatter\n",
    "    if 'Complexity_Impact' in func_df.columns:\n",
    "        scatter = ax3.scatter(func_df['Usage_Count'], func_df['Complexity_Impact'],\n",
    "                             alpha=0.7, s=100, color='coral')\n",
    "        ax3.set_xlabel('Usage Count')\n",
    "        ax3.set_ylabel('Complexity Impact (1-5)')\n",
    "        ax3.set_title('‚ö° Usage vs Complexity')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Function categories (if available)\n",
    "    ax4.text(0.1, 0.8, \"üéØ KEY MIGRATION INSIGHTS:\", transform=ax4.transAxes, \n",
    "             fontsize=12, fontweight='bold')\n",
    "    \n",
    "    insights = [\n",
    "        \"‚Ä¢ Focus on high-usage complex functions first\",\n",
    "        \"‚Ä¢ Test modified functions thoroughly\", \n",
    "        \"‚Ä¢ Consider performance implications\",\n",
    "        \"‚Ä¢ Plan for user training on new syntax\"\n",
    "    ]\n",
    "    \n",
    "    for i, insight in enumerate(insights):\n",
    "        ax4.text(0.1, 0.6 - i*0.1, insight, transform=ax4.transAxes, fontsize=10)\n",
    "    \n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Function usage data not available\")\n",
    "    print(\"üí° In a real assessment, this would show:\")\n",
    "    print(\"   ‚Ä¢ SQL Server specific functions used\")\n",
    "    print(\"   ‚Ä¢ Databricks compatibility status\")\n",
    "    print(\"   ‚Ä¢ Migration complexity by function type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Migration Planning Dashboard\n",
    "\n",
    "This section provides actionable insights for planning the migration, including:\n",
    "- **Migration Wave Strategy** - Phased approach based on complexity and risk\n",
    "- **Resource Planning** - Team size and timeline estimates\n",
    "- **Risk Mitigation** - Specific recommendations for high-risk components\n",
    "- **Cost-Benefit Analysis** - Investment vs expected returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Complexity_Analysis' in sheets_data:\n",
    "    df = sheets_data['Complexity_Analysis']\n",
    "    \n",
    "    print(\"üìã GLOBALSUPPLY CORP - MIGRATION PLANNING STRATEGY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define migration waves based on complexity and risk\n",
    "    def assign_migration_wave(row):\n",
    "        \"\"\"\n",
    "        Assign components to migration waves based on complexity and risk\n",
    "        Wave 1: Low risk, low-medium complexity (Quick Wins)\n",
    "        Wave 2: Medium risk or higher complexity (Standard Migration)\n",
    "        Wave 3: High risk or very high complexity (Complex Components)\n",
    "        \"\"\"\n",
    "        if row['Risk_Level'] == 'Low' and row['Complexity_Score'] < 6:\n",
    "            return 'Wave 1 - Quick Wins'\n",
    "        elif row['Risk_Level'] == 'Medium' or (row['Risk_Level'] == 'Low' and row['Complexity_Score'] >= 6):\n",
    "            return 'Wave 2 - Standard Migration'\n",
    "        else:\n",
    "            return 'Wave 3 - Complex Components'\n",
    "    \n",
    "    df['Migration_Wave'] = df.apply(assign_migration_wave, axis=1)\n",
    "    \n",
    "    # Analyze migration waves\n",
    "    wave_analysis = df.groupby('Migration_Wave').agg({\n",
    "        'File_Name': 'count',\n",
    "        'Migration_Hours': 'sum', \n",
    "        'Complexity_Score': 'mean',\n",
    "        'Lines_of_Code': 'sum'\n",
    "    }).round(1)\n",
    "    \n",
    "    wave_analysis.columns = ['File_Count', 'Total_Hours', 'Avg_Complexity', 'Total_LOC']\n",
    "    wave_analysis = wave_analysis.reindex([\n",
    "        'Wave 1 - Quick Wins', \n",
    "        'Wave 2 - Standard Migration', \n",
    "        'Wave 3 - Complex Components'\n",
    "    ])\n",
    "    \n",
    "    print(\"üåä RECOMMENDED MIGRATION WAVE STRATEGY:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    wave_descriptions = {\n",
    "        'Wave 1 - Quick Wins': {\n",
    "            'emoji': 'üü¢',\n",
    "            'description': 'Low risk, straightforward migrations to build momentum',\n",
    "            'timeline': '2-4 weeks',\n",
    "            'team': '1-2 developers'\n",
    "        },\n",
    "        'Wave 2 - Standard Migration': {\n",
    "            'emoji': 'üü°', \n",
    "            'description': 'Standard complexity migrations with moderate risk',\n",
    "            'timeline': '4-8 weeks',\n",
    "            'team': '2-3 developers'\n",
    "        },\n",
    "        'Wave 3 - Complex Components': {\n",
    "            'emoji': 'üî¥',\n",
    "            'description': 'High complexity/risk components requiring expert attention',\n",
    "            'timeline': '6-12 weeks',\n",
    "            'team': '3-4 senior developers'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for wave, row in wave_analysis.iterrows():\n",
    "        if pd.isna(row['File_Count']):\n",
    "            continue\n",
    "            \n",
    "        wave_info = wave_descriptions.get(wave, {'emoji': '‚ö™', 'description': '', 'timeline': '', 'team': ''})\n",
    "        \n",
    "        print(f\"{wave_info['emoji']} {wave}:\")\n",
    "        print(f\"   üìÑ Components: {int(row['File_Count'])}\")\n",
    "        print(f\"   ‚è±Ô∏è  Total Hours: {int(row['Total_Hours'])}\")\n",
    "        print(f\"   üìä Avg Complexity: {row['Avg_Complexity']}/10\")\n",
    "        print(f\"   üìè Lines of Code: {int(row['Total_LOC']):,}\")\n",
    "        print(f\"   üìÖ Timeline: {wave_info['timeline']}\")\n",
    "        print(f\"   üë• Team Size: {wave_info['team']}\")\n",
    "        print(f\"   üí° Strategy: {wave_info['description']}\")\n",
    "        print()\n",
    "    \n",
    "    # High-risk component analysis\n",
    "    high_risk_files = df[df['Risk_Level'] == 'High']\n",
    "    \n",
    "    if len(high_risk_files) > 0:\n",
    "        print(\"‚ö†Ô∏è  HIGH-RISK COMPONENTS - SPECIAL ATTENTION REQUIRED:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for _, file_data in high_risk_files.iterrows():\n",
    "            print(f\"üî¥ {file_data['File_Name']}:\")\n",
    "            print(f\"   üéØ Complexity: {file_data['Complexity_Score']}/10\")\n",
    "            print(f\"   ‚è±Ô∏è  Effort: {file_data['Migration_Hours']} hours\")\n",
    "            print(f\"   üîß Features: {file_data.get('SQL_Features', 'Advanced SQL patterns')}\")\n",
    "            \n",
    "            # Specific recommendations based on complexity\n",
    "            if file_data['Complexity_Score'] > 9:\n",
    "                print(f\"   üí° Recommendation: Assign senior architect, plan proof-of-concept\")\n",
    "            else:\n",
    "                print(f\"   üí° Recommendation: Assign senior developer, thorough testing required\")\n",
    "            print()\n",
    "    \n",
    "    # Cost-benefit analysis\n",
    "    total_hours = df['Migration_Hours'].sum()\n",
    "    avg_hourly_rate = 150\n",
    "    total_cost = total_hours * avg_hourly_rate\n",
    "    \n",
    "    print(\"üí∞ COST-BENEFIT ANALYSIS:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"üìä Total Migration Effort: {total_hours} hours\")\n",
    "    print(f\"üíµ Estimated Cost: ${total_cost:,} (@ ${avg_hourly_rate}/hour)\")\n",
    "    print(f\"üìÖ Sequential Timeline: {total_hours/40:.1f} weeks (1 developer)\")\n",
    "    print(f\"üìÖ Parallel Timeline: {total_hours/160:.1f} weeks (4 developers)\")\n",
    "    print(f\"üìÖ Recommended Timeline: {total_hours/120:.1f} weeks (3 developers + coordination)\")\n",
    "    \n",
    "    # Expected benefits\n",
    "    print(\"\\nüìà EXPECTED BUSINESS BENEFITS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    benefits = [\n",
    "        (\"Query Performance\", \"3-5x improvement\", \"Faster analytics, better user experience\"),\n",
    "        (\"Analytics Capability\", \"Advanced ML/AI\", \"Predictive supply chain optimization\"),\n",
    "        (\"Infrastructure Cost\", \"20-30% reduction\", \"Cloud-native scaling and optimization\"),\n",
    "        (\"Time-to-Insight\", \"10x faster\", \"Natural language queries with Genie\"),\n",
    "        (\"Scalability\", \"Unlimited scale\", \"Handle peak loads without performance issues\"),\n",
    "        (\"Innovation Speed\", \"2x faster\", \"Rapid prototyping of new analytics\")\n",
    "    ]\n",
    "    \n",
    "    for benefit, improvement, description in benefits:\n",
    "        print(f\"üí° {benefit}: {improvement}\")\n",
    "        print(f\"   {description}\")\n",
    "        print()\n",
    "    \n",
    "    # ROI calculation\n",
    "    annual_savings = 200000  # Estimated annual savings\n",
    "    payback_months = (total_cost / (annual_savings / 12))\n",
    "    \n",
    "    print(f\"üí∞ ROI PROJECTION:\")\n",
    "    print(f\"   Annual Savings Estimate: ${annual_savings:,}\")\n",
    "    print(f\"   Payback Period: {payback_months:.1f} months\")\n",
    "    print(f\"   3-Year ROI: {((annual_savings * 3 - total_cost) / total_cost * 100):.0f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Migration planning data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Migration Wave Visualization\n",
    "\n",
    "Visual representation of the recommended migration strategy showing how components are distributed across waves and the associated effort and risk levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Complexity_Analysis' in sheets_data and 'Migration_Wave' in df.columns:\n",
    "    # Create comprehensive migration planning visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('GlobalSupply Corp - Migration Wave Strategy Dashboard', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Migration wave distribution\n",
    "    wave_counts = df['Migration_Wave'].value_counts()\n",
    "    wave_order = ['Wave 1 - Quick Wins', 'Wave 2 - Standard Migration', 'Wave 3 - Complex Components']\n",
    "    wave_counts = wave_counts.reindex(wave_order).fillna(0)\n",
    "    \n",
    "    colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "    bars1 = ax1.bar(range(len(wave_counts)), wave_counts.values, color=colors)\n",
    "    ax1.set_title('üåä Components by Migration Wave')\n",
    "    ax1.set_xlabel('Migration Wave')\n",
    "    ax1.set_ylabel('Number of Components')\n",
    "    ax1.set_xticks(range(len(wave_counts)))\n",
    "    ax1.set_xticklabels(['Wave 1\\nQuick Wins', 'Wave 2\\nStandard', 'Wave 3\\nComplex'], rotation=0)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars1, wave_counts.values):\n",
    "        if value > 0:\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                     str(int(value)), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Effort distribution by wave\n",
    "    wave_hours = df.groupby('Migration_Wave')['Migration_Hours'].sum().reindex(wave_order).fillna(0)\n",
    "    bars2 = ax2.bar(range(len(wave_hours)), wave_hours.values, color=colors)\n",
    "    ax2.set_title('‚è±Ô∏è Effort Distribution by Wave')\n",
    "    ax2.set_xlabel('Migration Wave')\n",
    "    ax2.set_ylabel('Migration Hours')\n",
    "    ax2.set_xticks(range(len(wave_hours)))\n",
    "    ax2.set_xticklabels(['Wave 1\\nQuick Wins', 'Wave 2\\nStandard', 'Wave 3\\nComplex'], rotation=0)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars2, wave_hours.values):\n",
    "        if value > 0:\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                     f'{int(value)}h', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Risk vs Effort scatter plot by wave\n",
    "    wave_colors = {'Wave 1 - Quick Wins': '#2ecc71', \n",
    "                   'Wave 2 - Standard Migration': '#f39c12',\n",
    "                   'Wave 3 - Complex Components': '#e74c3c'}\n",
    "    \n",
    "    for wave in df['Migration_Wave'].unique():\n",
    "        wave_data = df[df['Migration_Wave'] == wave]\n",
    "        ax3.scatter(wave_data['Complexity_Score'], wave_data['Migration_Hours'],\n",
    "                   label=wave.replace(' - ', '\\n'), color=wave_colors.get(wave, '#95a5a6'),\n",
    "                   alpha=0.7, s=100, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax3.set_xlabel('Complexity Score')\n",
    "    ax3.set_ylabel('Migration Hours')\n",
    "    ax3.set_title('üìä Risk vs Effort by Wave')\n",
    "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Timeline visualization\n",
    "    # Create a Gantt-like chart showing wave timelines\n",
    "    wave_timeline = {\n",
    "        'Wave 1 - Quick Wins': {'start': 0, 'duration': 4, 'color': '#2ecc71'},\n",
    "        'Wave 2 - Standard Migration': {'start': 2, 'duration': 8, 'color': '#f39c12'},\n",
    "        'Wave 3 - Complex Components': {'start': 6, 'duration': 12, 'color': '#e74c3c'}\n",
    "    }\n",
    "    \n",
    "    y_pos = 0\n",
    "    for wave, timeline in wave_timeline.items():\n",
    "        ax4.barh(y_pos, timeline['duration'], left=timeline['start'], \n",
    "                color=timeline['color'], alpha=0.7, height=0.6)\n",
    "        \n",
    "        # Add wave labels\n",
    "        ax4.text(timeline['start'] + timeline['duration']/2, y_pos,\n",
    "                wave.split(' - ')[0], ha='center', va='center', \n",
    "                fontweight='bold', color='white')\n",
    "        y_pos += 1\n",
    "    \n",
    "    ax4.set_xlim(0, 18)\n",
    "    ax4.set_ylim(-0.5, 2.5)\n",
    "    ax4.set_xlabel('Timeline (Weeks)')\n",
    "    ax4.set_title('üìÖ Recommended Migration Timeline')\n",
    "    ax4.set_yticks(range(3))\n",
    "    ax4.set_yticklabels(['Wave 1', 'Wave 2', 'Wave 3'])\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add milestone markers\n",
    "    milestones = [(4, 'Wave 1 Complete'), (10, 'Wave 2 Complete'), (18, 'Migration Complete')]\n",
    "    for week, milestone in milestones:\n",
    "        ax4.axvline(x=week, color='red', linestyle='--', alpha=0.5)\n",
    "        ax4.text(week, 2.2, milestone, rotation=45, ha='right', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìã MIGRATION WAVE SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_components = len(df)\n",
    "    for wave in wave_order:\n",
    "        wave_data = df[df['Migration_Wave'] == wave]\n",
    "        if len(wave_data) > 0:\n",
    "            count = len(wave_data)\n",
    "            percentage = (count / total_components * 100)\n",
    "            avg_complexity = wave_data['Complexity_Score'].mean()\n",
    "            total_effort = wave_data['Migration_Hours'].sum()\n",
    "            \n",
    "            print(f\"{wave}:\")\n",
    "            print(f\"  üìä {count} components ({percentage:.1f}% of total)\")\n",
    "            print(f\"  üìà Average complexity: {avg_complexity:.1f}/10\")\n",
    "            print(f\"  ‚è±Ô∏è  Total effort: {total_effort} hours\")\n",
    "            print()\n",
    "            \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Migration wave data not available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Executive Summary & Next Steps\n",
    "\n",
    "Based on the comprehensive assessment analysis, here are the key findings and actionable next steps for GlobalSupply Corp's data modernization journey to Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary and actionable recommendations\n",
    "if 'Complexity_Analysis' in sheets_data:\n",
    "    df = sheets_data['Complexity_Analysis']\n",
    "    \n",
    "    # Calculate key metrics for summary\n",
    "    total_files = len(df)\n",
    "    total_hours = df['Migration_Hours'].sum()\n",
    "    total_cost = total_hours * 150\n",
    "    high_risk_count = len(df[df['Risk_Level'] == 'High'])\n",
    "    avg_complexity = df['Complexity_Score'].mean()\n",
    "    \n",
    "    print(\"\"\"\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üìä GLOBALSUPPLY CORP - EXECUTIVE SUMMARY & STRATEGIC RECOMMENDATIONS\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "üéØ ASSESSMENT FINDINGS:\n",
    "\"\"\")\n",
    "\n",
    "    # Key findings with specific numbers\n",
    "    findings = [\n",
    "        f\"‚Ä¢ {total_files} SQL components analyzed with {df['Lines_of_Code'].sum():,} lines of code\",\n",
    "        f\"‚Ä¢ Average complexity score: {avg_complexity:.1f}/10 (moderate-to-high complexity)\",\n",
    "        f\"‚Ä¢ {high_risk_count} high-risk components requiring expert attention\",\n",
    "        f\"‚Ä¢ Estimated migration effort: {total_hours} hours (${total_cost:,})\",\n",
    "        f\"‚Ä¢ Strong dependency relationships requiring careful sequencing\"\n",
    "    ]\n",
    "    \n",
    "    for finding in findings:\n",
    "        print(finding)\n",
    "\n",
    "    print(\"\"\"\n",
    "üìà BUSINESS IMPACT & ROI:\n",
    "‚Ä¢ Query Performance: 3-5x improvement for analytical workloads\n",
    "‚Ä¢ Natural Language Queries: Enable business users with Databricks Genie\n",
    "‚Ä¢ ML/AI Capabilities: Advanced supply chain optimization and forecasting\n",
    "‚Ä¢ Infrastructure Costs: 20-30% reduction through cloud-native optimization\n",
    "‚Ä¢ Time-to-Insight: 10x faster analytics development and deployment\n",
    "‚Ä¢ Scalability: Unlimited scale for peak demand scenarios\n",
    "\n",
    "üí∞ FINANCIAL PROJECTIONS:\n",
    "‚Ä¢ Investment Required: ${:,}\n",
    "‚Ä¢ Expected Annual Savings: $200,000+\n",
    "‚Ä¢ Payback Period: {:.1f} months\n",
    "‚Ä¢ 3-Year ROI: {}%\"\"\".format(\n",
    "        total_cost, \n",
    "        (total_cost / (200000 / 12)), \n",
    "        int(((200000 * 3 - total_cost) / total_cost * 100))\n",
    "    ))\n",
    "\n",
    "    print(\"\"\"\n",
    "üöÄ STRATEGIC RECOMMENDATIONS:\n",
    "\n",
    "1. ‚úÖ IMMEDIATE ACTIONS (Next 2 weeks):\n",
    "   ‚Üí Secure executive sponsorship and budget approval\n",
    "   ‚Üí Assemble migration team with SQL Server + Databricks expertise\n",
    "   ‚Üí Set up Databricks workspace and development environment\n",
    "   ‚Üí Begin with Module 2: Schema Migration & Transpilation workshop\n",
    "\n",
    "2. üìã SHORT-TERM EXECUTION (4-6 weeks):\n",
    "   ‚Üí Execute Wave 1 migrations (low complexity, quick wins)\n",
    "   ‚Üí Establish CI/CD pipelines for automated testing\n",
    "   ‚Üí Begin user training on Databricks platform\n",
    "   ‚Üí Proceed to Module 3: Data Reconciliation workshop\n",
    "\n",
    "3. üéØ MEDIUM-TERM DELIVERY (2-3 months):\n",
    "   ‚Üí Complete Wave 2 & 3 migrations with thorough testing\n",
    "   ‚Üí Implement advanced analytics and ML models\n",
    "   ‚Üí Deploy natural language query capabilities\n",
    "   ‚Üí Complete Module 4: Modern Analytics & ML workshop\n",
    "\n",
    "4. üåü LONG-TERM OPTIMIZATION (3-6 months):\n",
    "   ‚Üí Optimize performance and cost efficiency\n",
    "   ‚Üí Expand ML/AI use cases across supply chain\n",
    "   ‚Üí Train business users on self-service analytics\n",
    "   ‚Üí Plan for additional data sources and use cases\n",
    "\n",
    "üõ†Ô∏è CRITICAL SUCCESS FACTORS:\n",
    "‚Ä¢ Strong project management with clear milestones\n",
    "‚Ä¢ Dedicated team with both SQL Server and Databricks skills\n",
    "‚Ä¢ Comprehensive testing strategy including data validation\n",
    "‚Ä¢ User training and change management program\n",
    "‚Ä¢ Phased rollout with fallback procedures\n",
    "\n",
    "üìû RECOMMENDED SUPPORT RESOURCES:\n",
    "‚Ä¢ Databricks Professional Services for complex components\n",
    "‚Ä¢ Lakebridge community and documentation\n",
    "‚Ä¢ Partner ecosystem for specialized migration expertise\n",
    "‚Ä¢ Training programs for team skill development\"\"\")\n",
    "\n",
    "    # Risk mitigation strategies\n",
    "    if high_risk_count > 0:\n",
    "        print(f\"\"\"\n",
    "‚ö†Ô∏è  RISK MITIGATION FOR {high_risk_count} HIGH-RISK COMPONENTS:\n",
    "‚Ä¢ Assign senior architects to complex components\n",
    "‚Ä¢ Develop proof-of-concepts for high-risk migrations\n",
    "‚Ä¢ Plan for manual testing and validation\n",
    "‚Ä¢ Consider parallel runs during transition period\n",
    "‚Ä¢ Maintain rollback procedures for critical systems\"\"\")\n",
    "\n",
    "    print(\"\"\"\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üéØ DECISION: PROCEED WITH DATABRICKS MIGRATION\n",
    "\n",
    "The assessment demonstrates a strong business case for migrating GlobalSupply Corp's\n",
    "data warehouse to Databricks. The combination of performance improvements, cost \n",
    "savings, and advanced analytics capabilities provides compelling ROI.\n",
    "\n",
    "Next Workshop Module: Schema Migration & Transpilation\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Assessment data not available for executive summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Export Results for Stakeholders\n",
    "\n",
    "Generate reports for different stakeholder groups:\n",
    "- **Executive Summary** - High-level findings and recommendations\n",
    "- **Technical Report** - Detailed migration plan with component breakdown\n",
    "- **Project Plan** - Timeline, resources, and milestone tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comprehensive results for different stakeholder groups\n",
    "if 'Complexity_Analysis' in sheets_data:\n",
    "    df = sheets_data['Complexity_Analysis']\n",
    "    \n",
    "    # Ensure migration wave assignments exist\n",
    "    if 'Migration_Wave' not in df.columns:\n",
    "        def assign_migration_wave(row):\n",
    "            if row['Risk_Level'] == 'Low' and row['Complexity_Score'] < 6:\n",
    "                return 'Wave 1 - Quick Wins'\n",
    "            elif row['Risk_Level'] == 'Medium' or (row['Risk_Level'] == 'Low' and row['Complexity_Score'] >= 6):\n",
    "                return 'Wave 2 - Standard Migration'\n",
    "            else:\n",
    "                return 'Wave 3 - Complex Components'\n",
    "        df['Migration_Wave'] = df.apply(assign_migration_wave, axis=1)\n",
    "    \n",
    "    # 1. Export detailed technical migration plan\n",
    "    try:\n",
    "        df.to_csv('globalsupply_detailed_migration_plan.csv', index=False)\n",
    "        print(\"‚úÖ Detailed technical plan exported: globalsupply_detailed_migration_plan.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not export CSV: {e}\")\n",
    "    \n",
    "    # 2. Create executive summary document\n",
    "    total_files = len(df)\n",
    "    total_hours = df['Migration_Hours'].sum()\n",
    "    total_cost = total_hours * 150\n",
    "    high_risk_count = len(df[df['Risk_Level'] == 'High'])\n",
    "    \n",
    "    exec_summary = f\"\"\"\n",
    "GLOBALSUPPLY CORP - DATABRICKS MIGRATION ASSESSMENT\n",
    "Executive Summary Report\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "========================================================\n",
    "\n",
    "PROJECT SCOPE:\n",
    "‚Ä¢ {total_files} SQL components analyzed\n",
    "‚Ä¢ {df['Lines_of_Code'].sum():,} total lines of code\n",
    "‚Ä¢ {df['Table_References'].sum()} database table dependencies\n",
    "‚Ä¢ Supply chain analytics and reporting workloads\n",
    "\n",
    "INVESTMENT REQUIRED:\n",
    "‚Ä¢ Development Effort: {total_hours} hours\n",
    "‚Ä¢ Estimated Cost: ${total_cost:,} (including team, tools, training)\n",
    "‚Ä¢ Timeline: {total_hours/120:.1f} weeks with 3-person team\n",
    "‚Ä¢ Phased Approach: 3 migration waves over 4-5 months\n",
    "\n",
    "RISK ASSESSMENT:\n",
    "‚Ä¢ {high_risk_count} high-risk components requiring expert attention\n",
    "‚Ä¢ {len(df[df['Risk_Level'] == 'Medium'])} medium-risk components for standard migration\n",
    "‚Ä¢ {len(df[df['Risk_Level'] == 'Low'])} low-risk components for quick wins\n",
    "‚Ä¢ Comprehensive dependency mapping completed\n",
    "‚Ä¢ Mitigation strategies defined for all risk categories\n",
    "\n",
    "BUSINESS BENEFITS:\n",
    "‚Ä¢ Performance: 3-5x improvement in query execution\n",
    "‚Ä¢ Analytics: Advanced ML/AI capabilities for supply chain optimization\n",
    "‚Ä¢ User Experience: Natural language queries with Databricks Genie\n",
    "‚Ä¢ Cost Efficiency: 20-30% reduction in infrastructure costs\n",
    "‚Ä¢ Scalability: Unlimited scale for peak demand periods\n",
    "‚Ä¢ Innovation: 2x faster development of new analytics\n",
    "\n",
    "FINANCIAL PROJECTIONS:\n",
    "‚Ä¢ Annual Cost Savings: $200,000+ (infrastructure + productivity)\n",
    "‚Ä¢ Payback Period: {(total_cost / (200000 / 12)):.1f} months\n",
    "‚Ä¢ 3-Year Net Present Value: ${(200000 * 3 - total_cost):,}\n",
    "‚Ä¢ ROI: {((200000 * 3 - total_cost) / total_cost * 100):.0f}% over 3 years\n",
    "\n",
    "STRATEGIC RECOMMENDATION:\n",
    "PROCEED with Databricks migration using Lakebridge toolchain.\n",
    "The analysis demonstrates strong business justification with\n",
    "manageable technical risk and clear path to success.\n",
    "\n",
    "KEY SUCCESS FACTORS:\n",
    "‚Ä¢ Executive sponsorship and dedicated team\n",
    "‚Ä¢ Phased migration approach starting with quick wins\n",
    "‚Ä¢ Comprehensive testing and validation procedures\n",
    "‚Ä¢ User training and change management program\n",
    "‚Ä¢ Partnership with Databricks Professional Services\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Secure budget approval and team assignment\n",
    "2. Set up Databricks workspace and development environment  \n",
    "3. Begin Schema Migration & Transpilation workshop (Module 2)\n",
    "4. Execute Wave 1 migrations within 4-6 weeks\n",
    "\n",
    "This assessment provides the foundation for a successful\n",
    "modernization that will transform GlobalSupply Corp's \n",
    "analytics capabilities and competitive advantage.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open('globalsupply_executive_summary.txt', 'w') as f:\n",
    "            f.write(exec_summary)\n",
    "        print(\"‚úÖ Executive summary exported: globalsupply_executive_summary.txt\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not export executive summary: {e}\")\n",
    "    \n",
    "    # 3. Create project tracking template\n",
    "    if 'Migration_Wave' in df.columns:\n",
    "        wave_summary = df.groupby('Migration_Wave').agg({\n",
    "            'File_Name': 'count',\n",
    "            'Migration_Hours': 'sum',\n",
    "            'Complexity_Score': 'mean'\n",
    "        }).round(1)\n",
    "        \n",
    "        try:\n",
    "            wave_summary.to_csv('globalsupply_project_waves.csv')\n",
    "            print(\"‚úÖ Project wave summary exported: globalsupply_project_waves.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not export wave summary: {e}\")\n",
    "    \n",
    "    print(\"\\nüìä ASSESSMENT COMPLETE - Ready for Next Phase\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"üéØ Deliverables Generated:\")\n",
    "    print(\"   üìÑ Executive Summary (for leadership team)\")\n",
    "    print(\"   üìã Detailed Migration Plan (for technical team)\")\n",
    "    print(\"   üìä Project Wave Summary (for project managers)\")\n",
    "    print(\"\\nüöÄ Next Workshop Module: Schema Migration & Transpilation\")\n",
    "    print(\"   Location: ../02_transpilation/\")\n",
    "    print(\"   Focus: Hands-on SQL conversion using Lakebridge\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No assessment data available for export\")\n",
    "    print(\"\\nüìù To generate real assessment data:\")\n",
    "    print(\"   1. Export your SQL Server workloads to files\")\n",
    "    print(\"   2. Run: databricks labs lakebridge analyze --source-tech mssql\")\n",
    "    print(\"   3. Re-run this notebook with the generated Excel report\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}